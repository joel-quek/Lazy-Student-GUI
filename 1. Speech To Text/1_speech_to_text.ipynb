{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Speech to Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Code Converts Videos into Transcribed Text Documents using the Open AI Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/openai/whisper\n",
    "\n",
    "This is the original documentation for **Whisper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model('base')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Youtube Video to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install pytube"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.youtube.com/watch?v=EMlM6QTzJo0\n",
    "\n",
    "Title: CODE WITH ME | Build in Python a YouTube Downloader | How To Build YouTube Downloader Using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ytdownloader import Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link = input(\"Put your YouTube link here:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put your YouTube link here:\n",
      "Download Successful!\n"
     ]
    }
   ],
   "source": [
    "Download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: If it doesn't work, RESTART Jupyter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribe the Youtube Video File"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.youtube.com/watch?v=HbY51mVKrcE\n",
    "\n",
    "Title: OpenAI Whisper Demo: Convert Speech to Text in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" Hey, what's up coders? Welcome to one little coder. The best speech to text in Python in 2022, I would say is open AI whisper. In less than three lines of Python code, you could have state of the art, machine learning, state of the art, deep learning model that can do ASR, automatic speech recognition or speech to text. In this video, I'm going to show you how you can have state of the art speech to text using Python in 2022. And let's get started. The first thing is this Google call up notebook will be in the YouTube description. All you have to do is expand the YouTube description, see the Google call up notebook, click open it and then you'll get to go. But if not, then if you're going to create your own Google call up notebook, just make sure that you're running GPU. There are two ways to make sure that you're running GPU. One click runtime and then click change runtime and you can see GPU accelerator. If not, run the first line NVIDIA SMI that will give you the configuration of the GPU that you're running. Currently, I've got a Tesla T4 which is most likely what you would get if you're running on Google call up. Otherwise, you also you can check the RAM memory offered where I've got a 16-github. So we're going to see how to use open AI whisper to do speech to text in Python. The first step is for us to make installation of the library. It's just one line of code, pip install, git place and directly the git repository and I'm installing it in quiet mode. At this point, we have successfully installed the library whisper that will help us do speech to text. Once we have the library installed, which is whisper, then we have to load the library and then we have to also load the model. So we have to load the library and we have to load the model. So import whisper will import whisper library and then whisper to load underscore model, then you can specify the model here. Now, what is the model that you want to specify? And that is quite something that it depends upon what you want to do. So this is the model called you can see there is like five different types of model. Tiny, base, small, medium, large and you can also see each of these models, how many parameters they have got. If you are not familiar with deep learning models, the larger the number of size of parameters that you have got, the better or more accurate that these models would be. So you can see 39 million parameters, 74 million parameters for base, 244 million parameters for small, 769 million parameters of a medium, 1.5 billion parameters for the large model. And you can also see that they have got specifically, they have got special English only model in case if you want English only or if you want multi-lingual model, you can have multi-lingual. So tiny.en will give you English only model, tiny alone will give you multi-lingual model, which means you can do speech to text for a lot of languages that they have described here in this chart. And what is the RAM, be RAM requirement if you are running it on Google, sorry if you're running it on GPU, you can see the approximately around 1GB for tiny model, it goes all the way up to 10 gig for the large model and 5 gig for the medium model. And you can also see the improvement in the speed that you would get the relative speed. So if large model is going to take 1x, then you can see how much the medium size model would take, how much the small size model would take, how much the base model would take and how much the tiny model would take. So the tiny model would have like 32% or sorry 32 times more efficiency in terms of the model speed, especially the inference speed compared to the large model. So based on what you want the accuracy or the speed, you can decide what model that you want to go with. Right now at this point, I'm going to pick the medium model. Also, it is quite important for you to understand what is the type of language that you're going to use and see the word error rate, WER, the word error rate for that particular type of the language and then make a call about which model that you want to pick. Because I'm going to do just English, I'm going to go ahead with medium, which is a 760, I think a 769 million parameter model. So load the medium model. If you want to load something else like tiny, you can just change the line here, tiny or base or whatever, you can change the line here. So at this point, it's going to download the model. You can see that the model is a 1.4 to gigabits model and the model is successfully downloaded into your Google Collapse session. Now we are ready to do speech to text and as you know, for any speech to text, you need an audio file or a speech file that you're going to convert into text. For that, I'm going to download the audio file from the internet. For that, I'm just simply using the bash command WGet and I'm outputting it in the file name audio.mp3 and then I'm specifying the file here. So this comes from this website and this is this is particularly from a movie called Batman Begin by Christopher Nolan and it is particularly downloading this particular statement that says criminals thrive on the indulgence of societies understanding. I mean, as a native English speaker, this is quite difficult for me to understand or transcribe. Let me play this audio for you. Okay, so the same audio is downloaded here using WGet and once you download it, you can actually see the audio here in the folder and the next thing that you can do is play the same thing just to make sure that it is downloaded. Okay, at this point, we are ready to go to start our speech to text transcription. So we are going to do speech to text transcription and all you have to do is model dot transcribe and then do audio dot MP3. So model dot transcribe audio dot MP3 will give you the result and you can print the particular result, the text of the result. While you are printing this, while the model is transcribing, you can also get to know what language is this and also finally you can print the text. So when I print this, like you can see how long it takes, I am transcribing it audio dot MP3 and then the result is printed English. Criminals thrive on the indulgence of societies understanding. Now, at this point, you can stop the video, go ahead, build your own speech to text project. But if you want to know how the model sizes would differ and then that can have an impact, let me show you a quick demo. What I am going to do is I am going to just run the same thing. But instead of using a media model, I am going to use a tiny model. I am going to call it model underscore tiny or I can just simply call it tiny. And after I have the tiny model, I am going to use the same audio. I am not going to do anything different. I am going to use the same audio using tiny dot transcribe and when I say tiny and you can see that you downloaded so fast because the model is very small. And when I say tiny dot transcribe, you can see criminals thrive on the contents of societies understanding. You can see how the language is different first. It didn't understand criminals properly. There is an apostrophe S and also on the indulgence of societies understanding. So instead of indulgence, it says condense. So these words where there is either the noise or when the voice drops, the volume goes down. The speaker is not quite clear. You can see how the smaller size model performs versus the larger size model. Let us take another example. So I am going to go pick another example from the same movie. So you look very fashionable apart from the mud. I am going to copy the path. I am going to copy the path here. Copy the audio path. Come back here and then I am going to do the same thing that I just did last time. Oops. Yep. Mud.mp3. When we downloaded, we are going to save it still in the same file name audio.mp3. It's played. Oops. Did it not download? Let me delete this file just to make sure that this is gone. Come back here. Run this once again. Just to make sure that we are downloading the right file and played. You look very fashionable. And you can see the difference. Right? The first person who spoke spoken American accent. Now the person who spoke the spoken British accent. And this is Michael Kane. If you do not know. So now we are going to do transcription for that. First, we are going to use the larger model. In this case, we are going to use the model that is medium model. So it says you look very fashionable apart from the mud. I really suppose to be apart from the mud, but it says apart from the mud. Let's see what a tiny model does. It says you look very fashionable. Oh, pretty much. So you can see the last word was like the only one letter was the mistake in the previous model. But here you can see. Oh, pretty much. So you can see the huge difference. I would like to do one final test before we close the video. So this is the one. Bats are nocturnal. Bats might be, but even for billion playboys, three o'clock is pushing it. So I'm going to copy this and copy the audio address. Come back to the Google collab. First, delete the file in case, you know, if it is going to create any problem. Come back here, paste it, run this. So this is going to download the file after it downloads. I'm going to play it. I think I have to run it. I'm sorry. That's my mistake. Okay. Let's do the transcription. Bats are not pernil instead of saying nocturnal. Bats maybe, but even a billionaires playboy three o'clock is pushing him. Let's run this. English. That's your knock journal. That's my bite. But even for a billionaire playboy where the clock is pushing me, you can see the difference in the quality of transcription even for English when you change different models. But the point here is not to pick on the models, but the point here is you always have to make this call the trade of between the size of the model, the performance in when I say performance, the speed of the model's inference and also the error rate. So it's a call that you have to make, but it's my duty for me to tell you how the tiny model is performing versus how the large model is performing for the same audio clip. But if you just consider the medium model, which is not the largest model, this is the 769, 769 million parameter model. But even this model is quite amazing. It is not just simply amazing for the US accent, but you also saw that it was doing a good job for the UK accent, which is a British accent. And I myself have tested Indian accent with that. And then it has done a really good job even for Indian accent English, which is not something that I usually see with Google Assistant or Alexa or Siri. They usually struggle with Indian accent until, you know, it's really customized for Indian market. But otherwise, I usually see them struggling. But even then, this complete, like the simple open source model, open AI whisper is doing really a great job. So in 2022, if you want to build a speech to text solution using Python, I think you should turn towards opening a whisper. It's completely open source. You can see all the details about, you know, what is the error rate for, sorry, what is the error rate for each language? Word error rate, W here. What does the difference between each model and what other things that you can do? For example, translation and multilingual translation and language detection and all those things. And this, this particular project is aimed at somebody who doesn't know how to do speech to text. And the point is, just literally like three lines of Python code, like if you literally see the code, you import, then you load the model and then you can just literally, you know, transcribe and print the result. So basically in three lines of Python, let me run this for you. So I'm going to say three, sorry, three lines of Python code for speech to text. Okay. So what am I going to do? I'm going to say import whisper cool. Then model is equal to whisper.load underscore model. And I can say medium here. And then model dot transcribe of audio dot MP3. And then while I'm doing that, I can print the text. And when I run this, it should oops, there is load model. I'm so sorry load model. And while I'm running this, you can see at the end of this step, it is going to actually print the transcription of the audio. So in literally three lines of Python code, you can have a cutting edge state of the art speech to text model, a speech to text project in Python. And that is quite quite quite amazing. And I think you should definitely check this out. And if you are a university student trying to do something in speech to text, this is quite multi-lingual. Even if you're not trying to do something for English, there are a lot, lot more languages. You can see all the languages here. So I come from India, all these Indian languages are there, which is quite not not common with other other language models. So I would strongly recommend you to check this out. Any question, let me know in the comment section. Otherwise, see you in the next video. Happy coding.\",\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 5.84,\n",
       "   'text': \" Hey, what's up coders? Welcome to one little coder. The best speech to text in Python in\",\n",
       "   'tokens': [1911,\n",
       "    11,\n",
       "    437,\n",
       "    311,\n",
       "    493,\n",
       "    17656,\n",
       "    433,\n",
       "    30,\n",
       "    4027,\n",
       "    281,\n",
       "    472,\n",
       "    707,\n",
       "    17656,\n",
       "    260,\n",
       "    13,\n",
       "    440,\n",
       "    1151,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    294,\n",
       "    15329,\n",
       "    294],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.20689158969455296,\n",
       "   'compression_ratio': 1.7935222672064777,\n",
       "   'no_speech_prob': 0.13690988719463348},\n",
       "  {'id': 1,\n",
       "   'seek': 0,\n",
       "   'start': 5.84,\n",
       "   'end': 12.16,\n",
       "   'text': ' 2022, I would say is open AI whisper. In less than three lines of Python code, you could',\n",
       "   'tokens': [20229,\n",
       "    11,\n",
       "    286,\n",
       "    576,\n",
       "    584,\n",
       "    307,\n",
       "    1269,\n",
       "    7318,\n",
       "    26018,\n",
       "    13,\n",
       "    682,\n",
       "    1570,\n",
       "    813,\n",
       "    1045,\n",
       "    3876,\n",
       "    295,\n",
       "    15329,\n",
       "    3089,\n",
       "    11,\n",
       "    291,\n",
       "    727],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.20689158969455296,\n",
       "   'compression_ratio': 1.7935222672064777,\n",
       "   'no_speech_prob': 0.13690988719463348},\n",
       "  {'id': 2,\n",
       "   'seek': 0,\n",
       "   'start': 12.16,\n",
       "   'end': 17.2,\n",
       "   'text': ' have state of the art, machine learning, state of the art, deep learning model that can',\n",
       "   'tokens': [362,\n",
       "    1785,\n",
       "    295,\n",
       "    264,\n",
       "    1523,\n",
       "    11,\n",
       "    3479,\n",
       "    2539,\n",
       "    11,\n",
       "    1785,\n",
       "    295,\n",
       "    264,\n",
       "    1523,\n",
       "    11,\n",
       "    2452,\n",
       "    2539,\n",
       "    2316,\n",
       "    300,\n",
       "    393],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.20689158969455296,\n",
       "   'compression_ratio': 1.7935222672064777,\n",
       "   'no_speech_prob': 0.13690988719463348},\n",
       "  {'id': 3,\n",
       "   'seek': 0,\n",
       "   'start': 17.2,\n",
       "   'end': 22.8,\n",
       "   'text': \" do ASR, automatic speech recognition or speech to text. In this video, I'm going to show\",\n",
       "   'tokens': [360,\n",
       "    7469,\n",
       "    49,\n",
       "    11,\n",
       "    12509,\n",
       "    6218,\n",
       "    11150,\n",
       "    420,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    13,\n",
       "    682,\n",
       "    341,\n",
       "    960,\n",
       "    11,\n",
       "    286,\n",
       "    478,\n",
       "    516,\n",
       "    281,\n",
       "    855],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.20689158969455296,\n",
       "   'compression_ratio': 1.7935222672064777,\n",
       "   'no_speech_prob': 0.13690988719463348},\n",
       "  {'id': 4,\n",
       "   'seek': 0,\n",
       "   'start': 22.8,\n",
       "   'end': 29.32,\n",
       "   'text': \" you how you can have state of the art speech to text using Python in 2022. And let's get\",\n",
       "   'tokens': [291,\n",
       "    577,\n",
       "    291,\n",
       "    393,\n",
       "    362,\n",
       "    1785,\n",
       "    295,\n",
       "    264,\n",
       "    1523,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    1228,\n",
       "    15329,\n",
       "    294,\n",
       "    20229,\n",
       "    13,\n",
       "    400,\n",
       "    718,\n",
       "    311,\n",
       "    483],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.20689158969455296,\n",
       "   'compression_ratio': 1.7935222672064777,\n",
       "   'no_speech_prob': 0.13690988719463348},\n",
       "  {'id': 5,\n",
       "   'seek': 2932,\n",
       "   'start': 29.32,\n",
       "   'end': 33.56,\n",
       "   'text': ' started. The first thing is this Google call up notebook will be in the YouTube description.',\n",
       "   'tokens': [1409,\n",
       "    13,\n",
       "    440,\n",
       "    700,\n",
       "    551,\n",
       "    307,\n",
       "    341,\n",
       "    3329,\n",
       "    818,\n",
       "    493,\n",
       "    21060,\n",
       "    486,\n",
       "    312,\n",
       "    294,\n",
       "    264,\n",
       "    3088,\n",
       "    3855,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15554278237479074,\n",
       "   'compression_ratio': 1.9913793103448276,\n",
       "   'no_speech_prob': 0.002229535486549139},\n",
       "  {'id': 6,\n",
       "   'seek': 2932,\n",
       "   'start': 33.56,\n",
       "   'end': 38.04,\n",
       "   'text': ' All you have to do is expand the YouTube description, see the Google call up notebook,',\n",
       "   'tokens': [1057,\n",
       "    291,\n",
       "    362,\n",
       "    281,\n",
       "    360,\n",
       "    307,\n",
       "    5268,\n",
       "    264,\n",
       "    3088,\n",
       "    3855,\n",
       "    11,\n",
       "    536,\n",
       "    264,\n",
       "    3329,\n",
       "    818,\n",
       "    493,\n",
       "    21060,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15554278237479074,\n",
       "   'compression_ratio': 1.9913793103448276,\n",
       "   'no_speech_prob': 0.002229535486549139},\n",
       "  {'id': 7,\n",
       "   'seek': 2932,\n",
       "   'start': 38.04,\n",
       "   'end': 42.92,\n",
       "   'text': \" click open it and then you'll get to go. But if not, then if you're going to create your own\",\n",
       "   'tokens': [2052,\n",
       "    1269,\n",
       "    309,\n",
       "    293,\n",
       "    550,\n",
       "    291,\n",
       "    603,\n",
       "    483,\n",
       "    281,\n",
       "    352,\n",
       "    13,\n",
       "    583,\n",
       "    498,\n",
       "    406,\n",
       "    11,\n",
       "    550,\n",
       "    498,\n",
       "    291,\n",
       "    434,\n",
       "    516,\n",
       "    281,\n",
       "    1884,\n",
       "    428,\n",
       "    1065],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15554278237479074,\n",
       "   'compression_ratio': 1.9913793103448276,\n",
       "   'no_speech_prob': 0.002229535486549139},\n",
       "  {'id': 8,\n",
       "   'seek': 2932,\n",
       "   'start': 42.92,\n",
       "   'end': 47.56,\n",
       "   'text': \" Google call up notebook, just make sure that you're running GPU. There are two ways to make sure\",\n",
       "   'tokens': [3329,\n",
       "    818,\n",
       "    493,\n",
       "    21060,\n",
       "    11,\n",
       "    445,\n",
       "    652,\n",
       "    988,\n",
       "    300,\n",
       "    291,\n",
       "    434,\n",
       "    2614,\n",
       "    18407,\n",
       "    13,\n",
       "    821,\n",
       "    366,\n",
       "    732,\n",
       "    2098,\n",
       "    281,\n",
       "    652,\n",
       "    988],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15554278237479074,\n",
       "   'compression_ratio': 1.9913793103448276,\n",
       "   'no_speech_prob': 0.002229535486549139},\n",
       "  {'id': 9,\n",
       "   'seek': 2932,\n",
       "   'start': 47.56,\n",
       "   'end': 52.68,\n",
       "   'text': \" that you're running GPU. One click runtime and then click change runtime and you can see GPU\",\n",
       "   'tokens': [300,\n",
       "    291,\n",
       "    434,\n",
       "    2614,\n",
       "    18407,\n",
       "    13,\n",
       "    1485,\n",
       "    2052,\n",
       "    34474,\n",
       "    293,\n",
       "    550,\n",
       "    2052,\n",
       "    1319,\n",
       "    34474,\n",
       "    293,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    18407],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15554278237479074,\n",
       "   'compression_ratio': 1.9913793103448276,\n",
       "   'no_speech_prob': 0.002229535486549139},\n",
       "  {'id': 10,\n",
       "   'seek': 5268,\n",
       "   'start': 52.68,\n",
       "   'end': 60.12,\n",
       "   'text': ' accelerator. If not, run the first line NVIDIA SMI that will give you the configuration of the GPU',\n",
       "   'tokens': [39889,\n",
       "    13,\n",
       "    759,\n",
       "    406,\n",
       "    11,\n",
       "    1190,\n",
       "    264,\n",
       "    700,\n",
       "    1622,\n",
       "    426,\n",
       "    3958,\n",
       "    6914,\n",
       "    13115,\n",
       "    40,\n",
       "    300,\n",
       "    486,\n",
       "    976,\n",
       "    291,\n",
       "    264,\n",
       "    11694,\n",
       "    295,\n",
       "    264,\n",
       "    18407],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1804191195775592,\n",
       "   'compression_ratio': 1.5747508305647842,\n",
       "   'no_speech_prob': 9.597248572390527e-05},\n",
       "  {'id': 11,\n",
       "   'seek': 5268,\n",
       "   'start': 60.12,\n",
       "   'end': 64.52,\n",
       "   'text': \" that you're running. Currently, I've got a Tesla T4 which is most likely what you would get if\",\n",
       "   'tokens': [300,\n",
       "    291,\n",
       "    434,\n",
       "    2614,\n",
       "    13,\n",
       "    19964,\n",
       "    11,\n",
       "    286,\n",
       "    600,\n",
       "    658,\n",
       "    257,\n",
       "    13666,\n",
       "    314,\n",
       "    19,\n",
       "    597,\n",
       "    307,\n",
       "    881,\n",
       "    3700,\n",
       "    437,\n",
       "    291,\n",
       "    576,\n",
       "    483,\n",
       "    498],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1804191195775592,\n",
       "   'compression_ratio': 1.5747508305647842,\n",
       "   'no_speech_prob': 9.597248572390527e-05},\n",
       "  {'id': 12,\n",
       "   'seek': 5268,\n",
       "   'start': 64.52,\n",
       "   'end': 69.48,\n",
       "   'text': \" you're running on Google call up. Otherwise, you also you can check the RAM memory offered\",\n",
       "   'tokens': [291,\n",
       "    434,\n",
       "    2614,\n",
       "    322,\n",
       "    3329,\n",
       "    818,\n",
       "    493,\n",
       "    13,\n",
       "    10328,\n",
       "    11,\n",
       "    291,\n",
       "    611,\n",
       "    291,\n",
       "    393,\n",
       "    1520,\n",
       "    264,\n",
       "    14561,\n",
       "    4675,\n",
       "    8059],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1804191195775592,\n",
       "   'compression_ratio': 1.5747508305647842,\n",
       "   'no_speech_prob': 9.597248572390527e-05},\n",
       "  {'id': 13,\n",
       "   'seek': 5268,\n",
       "   'start': 69.48,\n",
       "   'end': 75.72,\n",
       "   'text': \" where I've got a 16-github. So we're going to see how to use open AI whisper to do speech to\",\n",
       "   'tokens': [689,\n",
       "    286,\n",
       "    600,\n",
       "    658,\n",
       "    257,\n",
       "    3165,\n",
       "    12,\n",
       "    70,\n",
       "    355,\n",
       "    836,\n",
       "    13,\n",
       "    407,\n",
       "    321,\n",
       "    434,\n",
       "    516,\n",
       "    281,\n",
       "    536,\n",
       "    577,\n",
       "    281,\n",
       "    764,\n",
       "    1269,\n",
       "    7318,\n",
       "    26018,\n",
       "    281,\n",
       "    360,\n",
       "    6218,\n",
       "    281],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1804191195775592,\n",
       "   'compression_ratio': 1.5747508305647842,\n",
       "   'no_speech_prob': 9.597248572390527e-05},\n",
       "  {'id': 14,\n",
       "   'seek': 5268,\n",
       "   'start': 75.72,\n",
       "   'end': 81.48,\n",
       "   'text': \" text in Python. The first step is for us to make installation of the library. It's just one line\",\n",
       "   'tokens': [2487,\n",
       "    294,\n",
       "    15329,\n",
       "    13,\n",
       "    440,\n",
       "    700,\n",
       "    1823,\n",
       "    307,\n",
       "    337,\n",
       "    505,\n",
       "    281,\n",
       "    652,\n",
       "    13260,\n",
       "    295,\n",
       "    264,\n",
       "    6405,\n",
       "    13,\n",
       "    467,\n",
       "    311,\n",
       "    445,\n",
       "    472,\n",
       "    1622],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1804191195775592,\n",
       "   'compression_ratio': 1.5747508305647842,\n",
       "   'no_speech_prob': 9.597248572390527e-05},\n",
       "  {'id': 15,\n",
       "   'seek': 8148,\n",
       "   'start': 81.48,\n",
       "   'end': 87.88000000000001,\n",
       "   'text': \" of code, pip install, git place and directly the git repository and I'm installing it in quiet mode.\",\n",
       "   'tokens': [295,\n",
       "    3089,\n",
       "    11,\n",
       "    8489,\n",
       "    3625,\n",
       "    11,\n",
       "    18331,\n",
       "    1081,\n",
       "    293,\n",
       "    3838,\n",
       "    264,\n",
       "    18331,\n",
       "    25841,\n",
       "    293,\n",
       "    286,\n",
       "    478,\n",
       "    20762,\n",
       "    309,\n",
       "    294,\n",
       "    5677,\n",
       "    4391,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10484352111816406,\n",
       "   'compression_ratio': 1.9949238578680204,\n",
       "   'no_speech_prob': 0.0005926850135438144},\n",
       "  {'id': 16,\n",
       "   'seek': 8148,\n",
       "   'start': 87.88000000000001,\n",
       "   'end': 95.4,\n",
       "   'text': ' At this point, we have successfully installed the library whisper that will help us do speech to text.',\n",
       "   'tokens': [1711,\n",
       "    341,\n",
       "    935,\n",
       "    11,\n",
       "    321,\n",
       "    362,\n",
       "    10727,\n",
       "    8899,\n",
       "    264,\n",
       "    6405,\n",
       "    26018,\n",
       "    300,\n",
       "    486,\n",
       "    854,\n",
       "    505,\n",
       "    360,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10484352111816406,\n",
       "   'compression_ratio': 1.9949238578680204,\n",
       "   'no_speech_prob': 0.0005926850135438144},\n",
       "  {'id': 17,\n",
       "   'seek': 8148,\n",
       "   'start': 95.4,\n",
       "   'end': 103.0,\n",
       "   'text': ' Once we have the library installed, which is whisper, then we have to load the library and then',\n",
       "   'tokens': [3443,\n",
       "    321,\n",
       "    362,\n",
       "    264,\n",
       "    6405,\n",
       "    8899,\n",
       "    11,\n",
       "    597,\n",
       "    307,\n",
       "    26018,\n",
       "    11,\n",
       "    550,\n",
       "    321,\n",
       "    362,\n",
       "    281,\n",
       "    3677,\n",
       "    264,\n",
       "    6405,\n",
       "    293,\n",
       "    550],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10484352111816406,\n",
       "   'compression_ratio': 1.9949238578680204,\n",
       "   'no_speech_prob': 0.0005926850135438144},\n",
       "  {'id': 18,\n",
       "   'seek': 8148,\n",
       "   'start': 103.0,\n",
       "   'end': 109.0,\n",
       "   'text': ' we have to also load the model. So we have to load the library and we have to load the model.',\n",
       "   'tokens': [321,\n",
       "    362,\n",
       "    281,\n",
       "    611,\n",
       "    3677,\n",
       "    264,\n",
       "    2316,\n",
       "    13,\n",
       "    407,\n",
       "    321,\n",
       "    362,\n",
       "    281,\n",
       "    3677,\n",
       "    264,\n",
       "    6405,\n",
       "    293,\n",
       "    321,\n",
       "    362,\n",
       "    281,\n",
       "    3677,\n",
       "    264,\n",
       "    2316,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10484352111816406,\n",
       "   'compression_ratio': 1.9949238578680204,\n",
       "   'no_speech_prob': 0.0005926850135438144},\n",
       "  {'id': 19,\n",
       "   'seek': 10900,\n",
       "   'start': 109.0,\n",
       "   'end': 115.8,\n",
       "   'text': ' So import whisper will import whisper library and then whisper to load underscore model,',\n",
       "   'tokens': [407,\n",
       "    974,\n",
       "    26018,\n",
       "    486,\n",
       "    974,\n",
       "    26018,\n",
       "    6405,\n",
       "    293,\n",
       "    550,\n",
       "    26018,\n",
       "    281,\n",
       "    3677,\n",
       "    37556,\n",
       "    2316,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14182385154392407,\n",
       "   'compression_ratio': 1.7897196261682242,\n",
       "   'no_speech_prob': 0.0003607578109949827},\n",
       "  {'id': 20,\n",
       "   'seek': 10900,\n",
       "   'start': 115.8,\n",
       "   'end': 122.12,\n",
       "   'text': ' then you can specify the model here. Now, what is the model that you want to specify? And that',\n",
       "   'tokens': [550,\n",
       "    291,\n",
       "    393,\n",
       "    16500,\n",
       "    264,\n",
       "    2316,\n",
       "    510,\n",
       "    13,\n",
       "    823,\n",
       "    11,\n",
       "    437,\n",
       "    307,\n",
       "    264,\n",
       "    2316,\n",
       "    300,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    16500,\n",
       "    30,\n",
       "    400,\n",
       "    300],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14182385154392407,\n",
       "   'compression_ratio': 1.7897196261682242,\n",
       "   'no_speech_prob': 0.0003607578109949827},\n",
       "  {'id': 21,\n",
       "   'seek': 10900,\n",
       "   'start': 122.12,\n",
       "   'end': 128.36,\n",
       "   'text': ' is quite something that it depends upon what you want to do. So this is the model called you can see',\n",
       "   'tokens': [307,\n",
       "    1596,\n",
       "    746,\n",
       "    300,\n",
       "    309,\n",
       "    5946,\n",
       "    3564,\n",
       "    437,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    360,\n",
       "    13,\n",
       "    407,\n",
       "    341,\n",
       "    307,\n",
       "    264,\n",
       "    2316,\n",
       "    1219,\n",
       "    291,\n",
       "    393,\n",
       "    536],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14182385154392407,\n",
       "   'compression_ratio': 1.7897196261682242,\n",
       "   'no_speech_prob': 0.0003607578109949827},\n",
       "  {'id': 22,\n",
       "   'seek': 10900,\n",
       "   'start': 128.36,\n",
       "   'end': 135.88,\n",
       "   'text': ' there is like five different types of model. Tiny, base, small, medium, large and you can also see',\n",
       "   'tokens': [456,\n",
       "    307,\n",
       "    411,\n",
       "    1732,\n",
       "    819,\n",
       "    3467,\n",
       "    295,\n",
       "    2316,\n",
       "    13,\n",
       "    39992,\n",
       "    11,\n",
       "    3096,\n",
       "    11,\n",
       "    1359,\n",
       "    11,\n",
       "    6399,\n",
       "    11,\n",
       "    2416,\n",
       "    293,\n",
       "    291,\n",
       "    393,\n",
       "    611,\n",
       "    536],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14182385154392407,\n",
       "   'compression_ratio': 1.7897196261682242,\n",
       "   'no_speech_prob': 0.0003607578109949827},\n",
       "  {'id': 23,\n",
       "   'seek': 13588,\n",
       "   'start': 135.88,\n",
       "   'end': 141.32,\n",
       "   'text': ' each of these models, how many parameters they have got. If you are not familiar with deep learning',\n",
       "   'tokens': [1184,\n",
       "    295,\n",
       "    613,\n",
       "    5245,\n",
       "    11,\n",
       "    577,\n",
       "    867,\n",
       "    9834,\n",
       "    436,\n",
       "    362,\n",
       "    658,\n",
       "    13,\n",
       "    759,\n",
       "    291,\n",
       "    366,\n",
       "    406,\n",
       "    4963,\n",
       "    365,\n",
       "    2452,\n",
       "    2539],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09435380160153567,\n",
       "   'compression_ratio': 1.9170731707317072,\n",
       "   'no_speech_prob': 0.0006956443539820611},\n",
       "  {'id': 24,\n",
       "   'seek': 13588,\n",
       "   'start': 141.32,\n",
       "   'end': 146.44,\n",
       "   'text': ' models, the larger the number of size of parameters that you have got, the better or more accurate',\n",
       "   'tokens': [5245,\n",
       "    11,\n",
       "    264,\n",
       "    4833,\n",
       "    264,\n",
       "    1230,\n",
       "    295,\n",
       "    2744,\n",
       "    295,\n",
       "    9834,\n",
       "    300,\n",
       "    291,\n",
       "    362,\n",
       "    658,\n",
       "    11,\n",
       "    264,\n",
       "    1101,\n",
       "    420,\n",
       "    544,\n",
       "    8559],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09435380160153567,\n",
       "   'compression_ratio': 1.9170731707317072,\n",
       "   'no_speech_prob': 0.0006956443539820611},\n",
       "  {'id': 25,\n",
       "   'seek': 13588,\n",
       "   'start': 146.44,\n",
       "   'end': 152.84,\n",
       "   'text': ' that these models would be. So you can see 39 million parameters, 74 million parameters for base,',\n",
       "   'tokens': [300,\n",
       "    613,\n",
       "    5245,\n",
       "    576,\n",
       "    312,\n",
       "    13,\n",
       "    407,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    15238,\n",
       "    2459,\n",
       "    9834,\n",
       "    11,\n",
       "    28868,\n",
       "    2459,\n",
       "    9834,\n",
       "    337,\n",
       "    3096,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09435380160153567,\n",
       "   'compression_ratio': 1.9170731707317072,\n",
       "   'no_speech_prob': 0.0006956443539820611},\n",
       "  {'id': 26,\n",
       "   'seek': 13588,\n",
       "   'start': 152.84,\n",
       "   'end': 162.28,\n",
       "   'text': ' 244 million parameters for small, 769 million parameters of a medium, 1.5 billion parameters for',\n",
       "   'tokens': [4022,\n",
       "    19,\n",
       "    2459,\n",
       "    9834,\n",
       "    337,\n",
       "    1359,\n",
       "    11,\n",
       "    24733,\n",
       "    24,\n",
       "    2459,\n",
       "    9834,\n",
       "    295,\n",
       "    257,\n",
       "    6399,\n",
       "    11,\n",
       "    502,\n",
       "    13,\n",
       "    20,\n",
       "    5218,\n",
       "    9834,\n",
       "    337],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.09435380160153567,\n",
       "   'compression_ratio': 1.9170731707317072,\n",
       "   'no_speech_prob': 0.0006956443539820611},\n",
       "  {'id': 27,\n",
       "   'seek': 16228,\n",
       "   'start': 162.28,\n",
       "   'end': 167.96,\n",
       "   'text': ' the large model. And you can also see that they have got specifically, they have got special',\n",
       "   'tokens': [264,\n",
       "    2416,\n",
       "    2316,\n",
       "    13,\n",
       "    400,\n",
       "    291,\n",
       "    393,\n",
       "    611,\n",
       "    536,\n",
       "    300,\n",
       "    436,\n",
       "    362,\n",
       "    658,\n",
       "    4682,\n",
       "    11,\n",
       "    436,\n",
       "    362,\n",
       "    658,\n",
       "    2121],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11141322583568339,\n",
       "   'compression_ratio': 1.9949238578680204,\n",
       "   'no_speech_prob': 0.000713111599907279},\n",
       "  {'id': 28,\n",
       "   'seek': 16228,\n",
       "   'start': 167.96,\n",
       "   'end': 174.04,\n",
       "   'text': ' English only model in case if you want English only or if you want multi-lingual model, you can have',\n",
       "   'tokens': [3669,\n",
       "    787,\n",
       "    2316,\n",
       "    294,\n",
       "    1389,\n",
       "    498,\n",
       "    291,\n",
       "    528,\n",
       "    3669,\n",
       "    787,\n",
       "    420,\n",
       "    498,\n",
       "    291,\n",
       "    528,\n",
       "    4825,\n",
       "    12,\n",
       "    1688,\n",
       "    901,\n",
       "    2316,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    362],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11141322583568339,\n",
       "   'compression_ratio': 1.9949238578680204,\n",
       "   'no_speech_prob': 0.000713111599907279},\n",
       "  {'id': 29,\n",
       "   'seek': 16228,\n",
       "   'start': 174.04,\n",
       "   'end': 181.64,\n",
       "   'text': ' multi-lingual. So tiny.en will give you English only model, tiny alone will give you multi-lingual',\n",
       "   'tokens': [4825,\n",
       "    12,\n",
       "    1688,\n",
       "    901,\n",
       "    13,\n",
       "    407,\n",
       "    5870,\n",
       "    13,\n",
       "    268,\n",
       "    486,\n",
       "    976,\n",
       "    291,\n",
       "    3669,\n",
       "    787,\n",
       "    2316,\n",
       "    11,\n",
       "    5870,\n",
       "    3312,\n",
       "    486,\n",
       "    976,\n",
       "    291,\n",
       "    4825,\n",
       "    12,\n",
       "    1688,\n",
       "    901],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11141322583568339,\n",
       "   'compression_ratio': 1.9949238578680204,\n",
       "   'no_speech_prob': 0.000713111599907279},\n",
       "  {'id': 30,\n",
       "   'seek': 16228,\n",
       "   'start': 181.64,\n",
       "   'end': 187.0,\n",
       "   'text': ' model, which means you can do speech to text for a lot of languages that they have described here in',\n",
       "   'tokens': [2316,\n",
       "    11,\n",
       "    597,\n",
       "    1355,\n",
       "    291,\n",
       "    393,\n",
       "    360,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    337,\n",
       "    257,\n",
       "    688,\n",
       "    295,\n",
       "    8650,\n",
       "    300,\n",
       "    436,\n",
       "    362,\n",
       "    7619,\n",
       "    510,\n",
       "    294],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11141322583568339,\n",
       "   'compression_ratio': 1.9949238578680204,\n",
       "   'no_speech_prob': 0.000713111599907279},\n",
       "  {'id': 31,\n",
       "   'seek': 18700,\n",
       "   'start': 187.0,\n",
       "   'end': 193.32,\n",
       "   'text': \" this chart. And what is the RAM, be RAM requirement if you are running it on Google, sorry if you're\",\n",
       "   'tokens': [341,\n",
       "    6927,\n",
       "    13,\n",
       "    400,\n",
       "    437,\n",
       "    307,\n",
       "    264,\n",
       "    14561,\n",
       "    11,\n",
       "    312,\n",
       "    14561,\n",
       "    11695,\n",
       "    498,\n",
       "    291,\n",
       "    366,\n",
       "    2614,\n",
       "    309,\n",
       "    322,\n",
       "    3329,\n",
       "    11,\n",
       "    2597,\n",
       "    498,\n",
       "    291,\n",
       "    434],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18503281621649714,\n",
       "   'compression_ratio': 1.6695278969957081,\n",
       "   'no_speech_prob': 0.00014526498853228986},\n",
       "  {'id': 32,\n",
       "   'seek': 18700,\n",
       "   'start': 193.32,\n",
       "   'end': 200.2,\n",
       "   'text': ' running it on GPU, you can see the approximately around 1GB for tiny model, it goes all the way up to',\n",
       "   'tokens': [2614,\n",
       "    309,\n",
       "    322,\n",
       "    18407,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    264,\n",
       "    10447,\n",
       "    926,\n",
       "    502,\n",
       "    8769,\n",
       "    337,\n",
       "    5870,\n",
       "    2316,\n",
       "    11,\n",
       "    309,\n",
       "    1709,\n",
       "    439,\n",
       "    264,\n",
       "    636,\n",
       "    493,\n",
       "    281],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18503281621649714,\n",
       "   'compression_ratio': 1.6695278969957081,\n",
       "   'no_speech_prob': 0.00014526498853228986},\n",
       "  {'id': 33,\n",
       "   'seek': 18700,\n",
       "   'start': 200.2,\n",
       "   'end': 206.84,\n",
       "   'text': ' 10 gig for the large model and 5 gig for the medium model. And you can also see the improvement',\n",
       "   'tokens': [1266,\n",
       "    8741,\n",
       "    337,\n",
       "    264,\n",
       "    2416,\n",
       "    2316,\n",
       "    293,\n",
       "    1025,\n",
       "    8741,\n",
       "    337,\n",
       "    264,\n",
       "    6399,\n",
       "    2316,\n",
       "    13,\n",
       "    400,\n",
       "    291,\n",
       "    393,\n",
       "    611,\n",
       "    536,\n",
       "    264,\n",
       "    10444],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18503281621649714,\n",
       "   'compression_ratio': 1.6695278969957081,\n",
       "   'no_speech_prob': 0.00014526498853228986},\n",
       "  {'id': 34,\n",
       "   'seek': 18700,\n",
       "   'start': 206.84,\n",
       "   'end': 212.92000000000002,\n",
       "   'text': ' in the speed that you would get the relative speed. So if large model is going to take 1x,',\n",
       "   'tokens': [294,\n",
       "    264,\n",
       "    3073,\n",
       "    300,\n",
       "    291,\n",
       "    576,\n",
       "    483,\n",
       "    264,\n",
       "    4972,\n",
       "    3073,\n",
       "    13,\n",
       "    407,\n",
       "    498,\n",
       "    2416,\n",
       "    2316,\n",
       "    307,\n",
       "    516,\n",
       "    281,\n",
       "    747,\n",
       "    502,\n",
       "    87,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.18503281621649714,\n",
       "   'compression_ratio': 1.6695278969957081,\n",
       "   'no_speech_prob': 0.00014526498853228986},\n",
       "  {'id': 35,\n",
       "   'seek': 21292,\n",
       "   'start': 212.92,\n",
       "   'end': 218.11999999999998,\n",
       "   'text': ' then you can see how much the medium size model would take, how much the small size model would take,',\n",
       "   'tokens': [550,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    577,\n",
       "    709,\n",
       "    264,\n",
       "    6399,\n",
       "    2744,\n",
       "    2316,\n",
       "    576,\n",
       "    747,\n",
       "    11,\n",
       "    577,\n",
       "    709,\n",
       "    264,\n",
       "    1359,\n",
       "    2744,\n",
       "    2316,\n",
       "    576,\n",
       "    747,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08458174191988431,\n",
       "   'compression_ratio': 2.0041322314049586,\n",
       "   'no_speech_prob': 3.264501356170513e-05},\n",
       "  {'id': 36,\n",
       "   'seek': 21292,\n",
       "   'start': 218.11999999999998,\n",
       "   'end': 223.16,\n",
       "   'text': ' how much the base model would take and how much the tiny model would take. So the tiny model would',\n",
       "   'tokens': [577,\n",
       "    709,\n",
       "    264,\n",
       "    3096,\n",
       "    2316,\n",
       "    576,\n",
       "    747,\n",
       "    293,\n",
       "    577,\n",
       "    709,\n",
       "    264,\n",
       "    5870,\n",
       "    2316,\n",
       "    576,\n",
       "    747,\n",
       "    13,\n",
       "    407,\n",
       "    264,\n",
       "    5870,\n",
       "    2316,\n",
       "    576],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08458174191988431,\n",
       "   'compression_ratio': 2.0041322314049586,\n",
       "   'no_speech_prob': 3.264501356170513e-05},\n",
       "  {'id': 37,\n",
       "   'seek': 21292,\n",
       "   'start': 223.16,\n",
       "   'end': 230.2,\n",
       "   'text': ' have like 32% or sorry 32 times more efficiency in terms of the model speed, especially the',\n",
       "   'tokens': [362,\n",
       "    411,\n",
       "    8858,\n",
       "    4,\n",
       "    420,\n",
       "    2597,\n",
       "    8858,\n",
       "    1413,\n",
       "    544,\n",
       "    10493,\n",
       "    294,\n",
       "    2115,\n",
       "    295,\n",
       "    264,\n",
       "    2316,\n",
       "    3073,\n",
       "    11,\n",
       "    2318,\n",
       "    264],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08458174191988431,\n",
       "   'compression_ratio': 2.0041322314049586,\n",
       "   'no_speech_prob': 3.264501356170513e-05},\n",
       "  {'id': 38,\n",
       "   'seek': 21292,\n",
       "   'start': 230.2,\n",
       "   'end': 236.04,\n",
       "   'text': ' inference speed compared to the large model. So based on what you want the accuracy or the speed,',\n",
       "   'tokens': [38253,\n",
       "    3073,\n",
       "    5347,\n",
       "    281,\n",
       "    264,\n",
       "    2416,\n",
       "    2316,\n",
       "    13,\n",
       "    407,\n",
       "    2361,\n",
       "    322,\n",
       "    437,\n",
       "    291,\n",
       "    528,\n",
       "    264,\n",
       "    14170,\n",
       "    420,\n",
       "    264,\n",
       "    3073,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08458174191988431,\n",
       "   'compression_ratio': 2.0041322314049586,\n",
       "   'no_speech_prob': 3.264501356170513e-05},\n",
       "  {'id': 39,\n",
       "   'seek': 21292,\n",
       "   'start': 236.04,\n",
       "   'end': 241.23999999999998,\n",
       "   'text': \" you can decide what model that you want to go with. Right now at this point, I'm going to pick\",\n",
       "   'tokens': [291,\n",
       "    393,\n",
       "    4536,\n",
       "    437,\n",
       "    2316,\n",
       "    300,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    352,\n",
       "    365,\n",
       "    13,\n",
       "    1779,\n",
       "    586,\n",
       "    412,\n",
       "    341,\n",
       "    935,\n",
       "    11,\n",
       "    286,\n",
       "    478,\n",
       "    516,\n",
       "    281,\n",
       "    1888],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08458174191988431,\n",
       "   'compression_ratio': 2.0041322314049586,\n",
       "   'no_speech_prob': 3.264501356170513e-05},\n",
       "  {'id': 40,\n",
       "   'seek': 24124,\n",
       "   'start': 241.24,\n",
       "   'end': 247.0,\n",
       "   'text': ' the medium model. Also, it is quite important for you to understand what is the type of language',\n",
       "   'tokens': [264,\n",
       "    6399,\n",
       "    2316,\n",
       "    13,\n",
       "    2743,\n",
       "    11,\n",
       "    309,\n",
       "    307,\n",
       "    1596,\n",
       "    1021,\n",
       "    337,\n",
       "    291,\n",
       "    281,\n",
       "    1223,\n",
       "    437,\n",
       "    307,\n",
       "    264,\n",
       "    2010,\n",
       "    295,\n",
       "    2856],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14136501312255859,\n",
       "   'compression_ratio': 1.6858407079646018,\n",
       "   'no_speech_prob': 0.00019245900330133736},\n",
       "  {'id': 41,\n",
       "   'seek': 24124,\n",
       "   'start': 247.0,\n",
       "   'end': 253.48000000000002,\n",
       "   'text': \" that you're going to use and see the word error rate, WER, the word error rate for that particular\",\n",
       "   'tokens': [300,\n",
       "    291,\n",
       "    434,\n",
       "    516,\n",
       "    281,\n",
       "    764,\n",
       "    293,\n",
       "    536,\n",
       "    264,\n",
       "    1349,\n",
       "    6713,\n",
       "    3314,\n",
       "    11,\n",
       "    343,\n",
       "    1598,\n",
       "    11,\n",
       "    264,\n",
       "    1349,\n",
       "    6713,\n",
       "    3314,\n",
       "    337,\n",
       "    300,\n",
       "    1729],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14136501312255859,\n",
       "   'compression_ratio': 1.6858407079646018,\n",
       "   'no_speech_prob': 0.00019245900330133736},\n",
       "  {'id': 42,\n",
       "   'seek': 24124,\n",
       "   'start': 253.48000000000002,\n",
       "   'end': 259.24,\n",
       "   'text': \" type of the language and then make a call about which model that you want to pick. Because I'm going\",\n",
       "   'tokens': [2010,\n",
       "    295,\n",
       "    264,\n",
       "    2856,\n",
       "    293,\n",
       "    550,\n",
       "    652,\n",
       "    257,\n",
       "    818,\n",
       "    466,\n",
       "    597,\n",
       "    2316,\n",
       "    300,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    1888,\n",
       "    13,\n",
       "    1436,\n",
       "    286,\n",
       "    478,\n",
       "    516],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14136501312255859,\n",
       "   'compression_ratio': 1.6858407079646018,\n",
       "   'no_speech_prob': 0.00019245900330133736},\n",
       "  {'id': 43,\n",
       "   'seek': 24124,\n",
       "   'start': 259.24,\n",
       "   'end': 266.6,\n",
       "   'text': \" to do just English, I'm going to go ahead with medium, which is a 760, I think a 769\",\n",
       "   'tokens': [281,\n",
       "    360,\n",
       "    445,\n",
       "    3669,\n",
       "    11,\n",
       "    286,\n",
       "    478,\n",
       "    516,\n",
       "    281,\n",
       "    352,\n",
       "    2286,\n",
       "    365,\n",
       "    6399,\n",
       "    11,\n",
       "    597,\n",
       "    307,\n",
       "    257,\n",
       "    1614,\n",
       "    4550,\n",
       "    11,\n",
       "    286,\n",
       "    519,\n",
       "    257,\n",
       "    1614,\n",
       "    30908],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14136501312255859,\n",
       "   'compression_ratio': 1.6858407079646018,\n",
       "   'no_speech_prob': 0.00019245900330133736},\n",
       "  {'id': 44,\n",
       "   'seek': 26660,\n",
       "   'start': 266.6,\n",
       "   'end': 272.20000000000005,\n",
       "   'text': ' million parameter model. So load the medium model. If you want to load something else like tiny,',\n",
       "   'tokens': [2459,\n",
       "    13075,\n",
       "    2316,\n",
       "    13,\n",
       "    407,\n",
       "    3677,\n",
       "    264,\n",
       "    6399,\n",
       "    2316,\n",
       "    13,\n",
       "    759,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    3677,\n",
       "    746,\n",
       "    1646,\n",
       "    411,\n",
       "    5870,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1481841912790507,\n",
       "   'compression_ratio': 1.7786259541984732,\n",
       "   'no_speech_prob': 0.000189958605915308},\n",
       "  {'id': 45,\n",
       "   'seek': 26660,\n",
       "   'start': 272.20000000000005,\n",
       "   'end': 277.24,\n",
       "   'text': ' you can just change the line here, tiny or base or whatever, you can change the line here.',\n",
       "   'tokens': [291,\n",
       "    393,\n",
       "    445,\n",
       "    1319,\n",
       "    264,\n",
       "    1622,\n",
       "    510,\n",
       "    11,\n",
       "    5870,\n",
       "    420,\n",
       "    3096,\n",
       "    420,\n",
       "    2035,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    1319,\n",
       "    264,\n",
       "    1622,\n",
       "    510,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1481841912790507,\n",
       "   'compression_ratio': 1.7786259541984732,\n",
       "   'no_speech_prob': 0.000189958605915308},\n",
       "  {'id': 46,\n",
       "   'seek': 26660,\n",
       "   'start': 277.24,\n",
       "   'end': 282.84000000000003,\n",
       "   'text': \" So at this point, it's going to download the model. You can see that the model is a 1.4 to\",\n",
       "   'tokens': [407,\n",
       "    412,\n",
       "    341,\n",
       "    935,\n",
       "    11,\n",
       "    309,\n",
       "    311,\n",
       "    516,\n",
       "    281,\n",
       "    5484,\n",
       "    264,\n",
       "    2316,\n",
       "    13,\n",
       "    509,\n",
       "    393,\n",
       "    536,\n",
       "    300,\n",
       "    264,\n",
       "    2316,\n",
       "    307,\n",
       "    257,\n",
       "    502,\n",
       "    13,\n",
       "    19,\n",
       "    281],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1481841912790507,\n",
       "   'compression_ratio': 1.7786259541984732,\n",
       "   'no_speech_prob': 0.000189958605915308},\n",
       "  {'id': 47,\n",
       "   'seek': 26660,\n",
       "   'start': 282.84000000000003,\n",
       "   'end': 288.12,\n",
       "   'text': ' gigabits model and the model is successfully downloaded into your Google Collapse session.',\n",
       "   'tokens': [8741,\n",
       "    455,\n",
       "    1208,\n",
       "    2316,\n",
       "    293,\n",
       "    264,\n",
       "    2316,\n",
       "    307,\n",
       "    10727,\n",
       "    21748,\n",
       "    666,\n",
       "    428,\n",
       "    3329,\n",
       "    4586,\n",
       "    11145,\n",
       "    5481,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1481841912790507,\n",
       "   'compression_ratio': 1.7786259541984732,\n",
       "   'no_speech_prob': 0.000189958605915308},\n",
       "  {'id': 48,\n",
       "   'seek': 26660,\n",
       "   'start': 288.68,\n",
       "   'end': 295.0,\n",
       "   'text': ' Now we are ready to do speech to text and as you know, for any speech to text, you need an audio',\n",
       "   'tokens': [823,\n",
       "    321,\n",
       "    366,\n",
       "    1919,\n",
       "    281,\n",
       "    360,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    293,\n",
       "    382,\n",
       "    291,\n",
       "    458,\n",
       "    11,\n",
       "    337,\n",
       "    604,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    11,\n",
       "    291,\n",
       "    643,\n",
       "    364,\n",
       "    6278],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1481841912790507,\n",
       "   'compression_ratio': 1.7786259541984732,\n",
       "   'no_speech_prob': 0.000189958605915308},\n",
       "  {'id': 49,\n",
       "   'seek': 29500,\n",
       "   'start': 295.0,\n",
       "   'end': 301.16,\n",
       "   'text': \" file or a speech file that you're going to convert into text. For that, I'm going to download\",\n",
       "   'tokens': [3991,\n",
       "    420,\n",
       "    257,\n",
       "    6218,\n",
       "    3991,\n",
       "    300,\n",
       "    291,\n",
       "    434,\n",
       "    516,\n",
       "    281,\n",
       "    7620,\n",
       "    666,\n",
       "    2487,\n",
       "    13,\n",
       "    1171,\n",
       "    300,\n",
       "    11,\n",
       "    286,\n",
       "    478,\n",
       "    516,\n",
       "    281,\n",
       "    5484],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12344748999482842,\n",
       "   'compression_ratio': 1.6790697674418604,\n",
       "   'no_speech_prob': 0.0004133323673158884},\n",
       "  {'id': 50,\n",
       "   'seek': 29500,\n",
       "   'start': 301.16,\n",
       "   'end': 307.16,\n",
       "   'text': \" the audio file from the internet. For that, I'm just simply using the bash command WGet\",\n",
       "   'tokens': [264,\n",
       "    6278,\n",
       "    3991,\n",
       "    490,\n",
       "    264,\n",
       "    4705,\n",
       "    13,\n",
       "    1171,\n",
       "    300,\n",
       "    11,\n",
       "    286,\n",
       "    478,\n",
       "    445,\n",
       "    2935,\n",
       "    1228,\n",
       "    264,\n",
       "    46183,\n",
       "    5622,\n",
       "    343,\n",
       "    18133],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12344748999482842,\n",
       "   'compression_ratio': 1.6790697674418604,\n",
       "   'no_speech_prob': 0.0004133323673158884},\n",
       "  {'id': 51,\n",
       "   'seek': 29500,\n",
       "   'start': 307.8,\n",
       "   'end': 312.84,\n",
       "   'text': \" and I'm outputting it in the file name audio.mp3 and then I'm specifying the file here.\",\n",
       "   'tokens': [293,\n",
       "    286,\n",
       "    478,\n",
       "    5598,\n",
       "    783,\n",
       "    309,\n",
       "    294,\n",
       "    264,\n",
       "    3991,\n",
       "    1315,\n",
       "    6278,\n",
       "    13,\n",
       "    2455,\n",
       "    18,\n",
       "    293,\n",
       "    550,\n",
       "    286,\n",
       "    478,\n",
       "    1608,\n",
       "    5489,\n",
       "    264,\n",
       "    3991,\n",
       "    510,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12344748999482842,\n",
       "   'compression_ratio': 1.6790697674418604,\n",
       "   'no_speech_prob': 0.0004133323673158884},\n",
       "  {'id': 52,\n",
       "   'seek': 29500,\n",
       "   'start': 312.84,\n",
       "   'end': 319.48,\n",
       "   'text': ' So this comes from this website and this is this is particularly from a movie called Batman',\n",
       "   'tokens': [407,\n",
       "    341,\n",
       "    1487,\n",
       "    490,\n",
       "    341,\n",
       "    3144,\n",
       "    293,\n",
       "    341,\n",
       "    307,\n",
       "    341,\n",
       "    307,\n",
       "    4098,\n",
       "    490,\n",
       "    257,\n",
       "    3169,\n",
       "    1219,\n",
       "    15432],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12344748999482842,\n",
       "   'compression_ratio': 1.6790697674418604,\n",
       "   'no_speech_prob': 0.0004133323673158884},\n",
       "  {'id': 53,\n",
       "   'seek': 31948,\n",
       "   'start': 319.48,\n",
       "   'end': 325.24,\n",
       "   'text': ' Begin by Christopher Nolan and it is particularly downloading this particular statement that says',\n",
       "   'tokens': [20660,\n",
       "    538,\n",
       "    20649,\n",
       "    43707,\n",
       "    293,\n",
       "    309,\n",
       "    307,\n",
       "    4098,\n",
       "    32529,\n",
       "    341,\n",
       "    1729,\n",
       "    5629,\n",
       "    300,\n",
       "    1619],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13805314472743443,\n",
       "   'compression_ratio': 1.6083333333333334,\n",
       "   'no_speech_prob': 0.0001602963893674314},\n",
       "  {'id': 54,\n",
       "   'seek': 31948,\n",
       "   'start': 325.24,\n",
       "   'end': 332.28000000000003,\n",
       "   'text': ' criminals thrive on the indulgence of societies understanding. I mean, as a native English speaker,',\n",
       "   'tokens': [23474,\n",
       "    21233,\n",
       "    322,\n",
       "    264,\n",
       "    28626,\n",
       "    15260,\n",
       "    295,\n",
       "    19329,\n",
       "    3701,\n",
       "    13,\n",
       "    286,\n",
       "    914,\n",
       "    11,\n",
       "    382,\n",
       "    257,\n",
       "    8470,\n",
       "    3669,\n",
       "    8145,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13805314472743443,\n",
       "   'compression_ratio': 1.6083333333333334,\n",
       "   'no_speech_prob': 0.0001602963893674314},\n",
       "  {'id': 55,\n",
       "   'seek': 31948,\n",
       "   'start': 332.28000000000003,\n",
       "   'end': 336.76,\n",
       "   'text': ' this is quite difficult for me to understand or transcribe. Let me play this audio for you.',\n",
       "   'tokens': [341,\n",
       "    307,\n",
       "    1596,\n",
       "    2252,\n",
       "    337,\n",
       "    385,\n",
       "    281,\n",
       "    1223,\n",
       "    420,\n",
       "    1145,\n",
       "    8056,\n",
       "    13,\n",
       "    961,\n",
       "    385,\n",
       "    862,\n",
       "    341,\n",
       "    6278,\n",
       "    337,\n",
       "    291,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13805314472743443,\n",
       "   'compression_ratio': 1.6083333333333334,\n",
       "   'no_speech_prob': 0.0001602963893674314},\n",
       "  {'id': 56,\n",
       "   'seek': 31948,\n",
       "   'start': 341.56,\n",
       "   'end': 348.28000000000003,\n",
       "   'text': ' Okay, so the same audio is downloaded here using WGet and once you download it, you can actually',\n",
       "   'tokens': [1033,\n",
       "    11,\n",
       "    370,\n",
       "    264,\n",
       "    912,\n",
       "    6278,\n",
       "    307,\n",
       "    21748,\n",
       "    510,\n",
       "    1228,\n",
       "    343,\n",
       "    18133,\n",
       "    293,\n",
       "    1564,\n",
       "    291,\n",
       "    5484,\n",
       "    309,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    767],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13805314472743443,\n",
       "   'compression_ratio': 1.6083333333333334,\n",
       "   'no_speech_prob': 0.0001602963893674314},\n",
       "  {'id': 57,\n",
       "   'seek': 34828,\n",
       "   'start': 348.28,\n",
       "   'end': 353.32,\n",
       "   'text': ' see the audio here in the folder and the next thing that you can do is play the same thing just to',\n",
       "   'tokens': [536,\n",
       "    264,\n",
       "    6278,\n",
       "    510,\n",
       "    294,\n",
       "    264,\n",
       "    10820,\n",
       "    293,\n",
       "    264,\n",
       "    958,\n",
       "    551,\n",
       "    300,\n",
       "    291,\n",
       "    393,\n",
       "    360,\n",
       "    307,\n",
       "    862,\n",
       "    264,\n",
       "    912,\n",
       "    551,\n",
       "    445,\n",
       "    281],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08715041696208797,\n",
       "   'compression_ratio': 1.7017543859649122,\n",
       "   'no_speech_prob': 7.397598528768867e-05},\n",
       "  {'id': 58,\n",
       "   'seek': 34828,\n",
       "   'start': 353.32,\n",
       "   'end': 365.96,\n",
       "   'text': ' make sure that it is downloaded. Okay, at this point, we are ready to go to start our speech to text',\n",
       "   'tokens': [652,\n",
       "    988,\n",
       "    300,\n",
       "    309,\n",
       "    307,\n",
       "    21748,\n",
       "    13,\n",
       "    1033,\n",
       "    11,\n",
       "    412,\n",
       "    341,\n",
       "    935,\n",
       "    11,\n",
       "    321,\n",
       "    366,\n",
       "    1919,\n",
       "    281,\n",
       "    352,\n",
       "    281,\n",
       "    722,\n",
       "    527,\n",
       "    6218,\n",
       "    281,\n",
       "    2487],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08715041696208797,\n",
       "   'compression_ratio': 1.7017543859649122,\n",
       "   'no_speech_prob': 7.397598528768867e-05},\n",
       "  {'id': 59,\n",
       "   'seek': 34828,\n",
       "   'start': 365.96,\n",
       "   'end': 372.28,\n",
       "   'text': ' transcription. So we are going to do speech to text transcription and all you have to do is',\n",
       "   'tokens': [35288,\n",
       "    13,\n",
       "    407,\n",
       "    321,\n",
       "    366,\n",
       "    516,\n",
       "    281,\n",
       "    360,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    35288,\n",
       "    293,\n",
       "    439,\n",
       "    291,\n",
       "    362,\n",
       "    281,\n",
       "    360,\n",
       "    307],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08715041696208797,\n",
       "   'compression_ratio': 1.7017543859649122,\n",
       "   'no_speech_prob': 7.397598528768867e-05},\n",
       "  {'id': 60,\n",
       "   'seek': 37228,\n",
       "   'start': 372.28,\n",
       "   'end': 381.08,\n",
       "   'text': ' model dot transcribe and then do audio dot MP3. So model dot transcribe audio dot MP3 will give you',\n",
       "   'tokens': [2316,\n",
       "    5893,\n",
       "    1145,\n",
       "    8056,\n",
       "    293,\n",
       "    550,\n",
       "    360,\n",
       "    6278,\n",
       "    5893,\n",
       "    14146,\n",
       "    18,\n",
       "    13,\n",
       "    407,\n",
       "    2316,\n",
       "    5893,\n",
       "    1145,\n",
       "    8056,\n",
       "    6278,\n",
       "    5893,\n",
       "    14146,\n",
       "    18,\n",
       "    486,\n",
       "    976,\n",
       "    291],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10852430284637765,\n",
       "   'compression_ratio': 1.8774509803921569,\n",
       "   'no_speech_prob': 0.00024810817558318377},\n",
       "  {'id': 61,\n",
       "   'seek': 37228,\n",
       "   'start': 381.08,\n",
       "   'end': 386.67999999999995,\n",
       "   'text': ' the result and you can print the particular result, the text of the result. While you are printing',\n",
       "   'tokens': [264,\n",
       "    1874,\n",
       "    293,\n",
       "    291,\n",
       "    393,\n",
       "    4482,\n",
       "    264,\n",
       "    1729,\n",
       "    1874,\n",
       "    11,\n",
       "    264,\n",
       "    2487,\n",
       "    295,\n",
       "    264,\n",
       "    1874,\n",
       "    13,\n",
       "    3987,\n",
       "    291,\n",
       "    366,\n",
       "    14699],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10852430284637765,\n",
       "   'compression_ratio': 1.8774509803921569,\n",
       "   'no_speech_prob': 0.00024810817558318377},\n",
       "  {'id': 62,\n",
       "   'seek': 37228,\n",
       "   'start': 386.67999999999995,\n",
       "   'end': 393.08,\n",
       "   'text': ' this, while the model is transcribing, you can also get to know what language is this and also',\n",
       "   'tokens': [341,\n",
       "    11,\n",
       "    1339,\n",
       "    264,\n",
       "    2316,\n",
       "    307,\n",
       "    1145,\n",
       "    39541,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    611,\n",
       "    483,\n",
       "    281,\n",
       "    458,\n",
       "    437,\n",
       "    2856,\n",
       "    307,\n",
       "    341,\n",
       "    293,\n",
       "    611],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10852430284637765,\n",
       "   'compression_ratio': 1.8774509803921569,\n",
       "   'no_speech_prob': 0.00024810817558318377},\n",
       "  {'id': 63,\n",
       "   'seek': 37228,\n",
       "   'start': 393.08,\n",
       "   'end': 397.64,\n",
       "   'text': ' finally you can print the text. So when I print this, like you can see how long it takes,',\n",
       "   'tokens': [2721,\n",
       "    291,\n",
       "    393,\n",
       "    4482,\n",
       "    264,\n",
       "    2487,\n",
       "    13,\n",
       "    407,\n",
       "    562,\n",
       "    286,\n",
       "    4482,\n",
       "    341,\n",
       "    11,\n",
       "    411,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    577,\n",
       "    938,\n",
       "    309,\n",
       "    2516,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10852430284637765,\n",
       "   'compression_ratio': 1.8774509803921569,\n",
       "   'no_speech_prob': 0.00024810817558318377},\n",
       "  {'id': 64,\n",
       "   'seek': 39764,\n",
       "   'start': 397.64,\n",
       "   'end': 402.84,\n",
       "   'text': ' I am transcribing it audio dot MP3 and then the result is printed English.',\n",
       "   'tokens': [286,\n",
       "    669,\n",
       "    1145,\n",
       "    39541,\n",
       "    309,\n",
       "    6278,\n",
       "    5893,\n",
       "    14146,\n",
       "    18,\n",
       "    293,\n",
       "    550,\n",
       "    264,\n",
       "    1874,\n",
       "    307,\n",
       "    13567,\n",
       "    3669,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1870461901028951,\n",
       "   'compression_ratio': 1.5101214574898785,\n",
       "   'no_speech_prob': 5.71451018913649e-05},\n",
       "  {'id': 65,\n",
       "   'seek': 39764,\n",
       "   'start': 402.84,\n",
       "   'end': 411.08,\n",
       "   'text': ' Criminals thrive on the indulgence of societies understanding. Now, at this point, you can stop the',\n",
       "   'tokens': [4779,\n",
       "    4395,\n",
       "    1124,\n",
       "    21233,\n",
       "    322,\n",
       "    264,\n",
       "    28626,\n",
       "    15260,\n",
       "    295,\n",
       "    19329,\n",
       "    3701,\n",
       "    13,\n",
       "    823,\n",
       "    11,\n",
       "    412,\n",
       "    341,\n",
       "    935,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    1590,\n",
       "    264],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1870461901028951,\n",
       "   'compression_ratio': 1.5101214574898785,\n",
       "   'no_speech_prob': 5.71451018913649e-05},\n",
       "  {'id': 66,\n",
       "   'seek': 39764,\n",
       "   'start': 411.08,\n",
       "   'end': 418.2,\n",
       "   'text': ' video, go ahead, build your own speech to text project. But if you want to know how the model sizes',\n",
       "   'tokens': [960,\n",
       "    11,\n",
       "    352,\n",
       "    2286,\n",
       "    11,\n",
       "    1322,\n",
       "    428,\n",
       "    1065,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    1716,\n",
       "    13,\n",
       "    583,\n",
       "    498,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    458,\n",
       "    577,\n",
       "    264,\n",
       "    2316,\n",
       "    11602],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1870461901028951,\n",
       "   'compression_ratio': 1.5101214574898785,\n",
       "   'no_speech_prob': 5.71451018913649e-05},\n",
       "  {'id': 67,\n",
       "   'seek': 39764,\n",
       "   'start': 418.2,\n",
       "   'end': 423.15999999999997,\n",
       "   'text': ' would differ and then that can have an impact, let me show you a quick demo. What I am going to do',\n",
       "   'tokens': [576,\n",
       "    743,\n",
       "    293,\n",
       "    550,\n",
       "    300,\n",
       "    393,\n",
       "    362,\n",
       "    364,\n",
       "    2712,\n",
       "    11,\n",
       "    718,\n",
       "    385,\n",
       "    855,\n",
       "    291,\n",
       "    257,\n",
       "    1702,\n",
       "    10723,\n",
       "    13,\n",
       "    708,\n",
       "    286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    360],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1870461901028951,\n",
       "   'compression_ratio': 1.5101214574898785,\n",
       "   'no_speech_prob': 5.71451018913649e-05},\n",
       "  {'id': 68,\n",
       "   'seek': 42316,\n",
       "   'start': 423.16,\n",
       "   'end': 430.20000000000005,\n",
       "   'text': ' is I am going to just run the same thing. But instead of using a media model, I am going to use a',\n",
       "   'tokens': [307,\n",
       "    286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    445,\n",
       "    1190,\n",
       "    264,\n",
       "    912,\n",
       "    551,\n",
       "    13,\n",
       "    583,\n",
       "    2602,\n",
       "    295,\n",
       "    1228,\n",
       "    257,\n",
       "    3021,\n",
       "    2316,\n",
       "    11,\n",
       "    286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    764,\n",
       "    257],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11688896892516594,\n",
       "   'compression_ratio': 2.079295154185022,\n",
       "   'no_speech_prob': 0.00019146206614095718},\n",
       "  {'id': 69,\n",
       "   'seek': 42316,\n",
       "   'start': 430.20000000000005,\n",
       "   'end': 436.12,\n",
       "   'text': ' tiny model. I am going to call it model underscore tiny or I can just simply call it tiny.',\n",
       "   'tokens': [5870,\n",
       "    2316,\n",
       "    13,\n",
       "    286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    818,\n",
       "    309,\n",
       "    2316,\n",
       "    37556,\n",
       "    5870,\n",
       "    420,\n",
       "    286,\n",
       "    393,\n",
       "    445,\n",
       "    2935,\n",
       "    818,\n",
       "    309,\n",
       "    5870,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11688896892516594,\n",
       "   'compression_ratio': 2.079295154185022,\n",
       "   'no_speech_prob': 0.00019146206614095718},\n",
       "  {'id': 70,\n",
       "   'seek': 42316,\n",
       "   'start': 436.12,\n",
       "   'end': 440.52000000000004,\n",
       "   'text': ' And after I have the tiny model, I am going to use the same audio. I am not going to do anything',\n",
       "   'tokens': [400,\n",
       "    934,\n",
       "    286,\n",
       "    362,\n",
       "    264,\n",
       "    5870,\n",
       "    2316,\n",
       "    11,\n",
       "    286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    764,\n",
       "    264,\n",
       "    912,\n",
       "    6278,\n",
       "    13,\n",
       "    286,\n",
       "    669,\n",
       "    406,\n",
       "    516,\n",
       "    281,\n",
       "    360,\n",
       "    1340],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11688896892516594,\n",
       "   'compression_ratio': 2.079295154185022,\n",
       "   'no_speech_prob': 0.00019146206614095718},\n",
       "  {'id': 71,\n",
       "   'seek': 42316,\n",
       "   'start': 440.52000000000004,\n",
       "   'end': 447.40000000000003,\n",
       "   'text': ' different. I am going to use the same audio using tiny dot transcribe and when I say tiny',\n",
       "   'tokens': [819,\n",
       "    13,\n",
       "    286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    764,\n",
       "    264,\n",
       "    912,\n",
       "    6278,\n",
       "    1228,\n",
       "    5870,\n",
       "    5893,\n",
       "    1145,\n",
       "    8056,\n",
       "    293,\n",
       "    562,\n",
       "    286,\n",
       "    584,\n",
       "    5870],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11688896892516594,\n",
       "   'compression_ratio': 2.079295154185022,\n",
       "   'no_speech_prob': 0.00019146206614095718},\n",
       "  {'id': 72,\n",
       "   'seek': 42316,\n",
       "   'start': 447.40000000000003,\n",
       "   'end': 451.72,\n",
       "   'text': ' and you can see that you downloaded so fast because the model is very small. And when I say tiny',\n",
       "   'tokens': [293,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    300,\n",
       "    291,\n",
       "    21748,\n",
       "    370,\n",
       "    2370,\n",
       "    570,\n",
       "    264,\n",
       "    2316,\n",
       "    307,\n",
       "    588,\n",
       "    1359,\n",
       "    13,\n",
       "    400,\n",
       "    562,\n",
       "    286,\n",
       "    584,\n",
       "    5870],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.11688896892516594,\n",
       "   'compression_ratio': 2.079295154185022,\n",
       "   'no_speech_prob': 0.00019146206614095718},\n",
       "  {'id': 73,\n",
       "   'seek': 45172,\n",
       "   'start': 451.72,\n",
       "   'end': 457.64000000000004,\n",
       "   'text': ' dot transcribe, you can see criminals thrive on the contents of societies understanding.',\n",
       "   'tokens': [5893,\n",
       "    1145,\n",
       "    8056,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    23474,\n",
       "    21233,\n",
       "    322,\n",
       "    264,\n",
       "    15768,\n",
       "    295,\n",
       "    19329,\n",
       "    3701,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14698275397805607,\n",
       "   'compression_ratio': 1.8067632850241546,\n",
       "   'no_speech_prob': 6.176982424221933e-05},\n",
       "  {'id': 74,\n",
       "   'seek': 45172,\n",
       "   'start': 457.64000000000004,\n",
       "   'end': 463.8,\n",
       "   'text': \" You can see how the language is different first. It didn't understand criminals properly. There\",\n",
       "   'tokens': [509,\n",
       "    393,\n",
       "    536,\n",
       "    577,\n",
       "    264,\n",
       "    2856,\n",
       "    307,\n",
       "    819,\n",
       "    700,\n",
       "    13,\n",
       "    467,\n",
       "    994,\n",
       "    380,\n",
       "    1223,\n",
       "    23474,\n",
       "    6108,\n",
       "    13,\n",
       "    821],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14698275397805607,\n",
       "   'compression_ratio': 1.8067632850241546,\n",
       "   'no_speech_prob': 6.176982424221933e-05},\n",
       "  {'id': 75,\n",
       "   'seek': 45172,\n",
       "   'start': 463.8,\n",
       "   'end': 469.72,\n",
       "   'text': ' is an apostrophe S and also on the indulgence of societies understanding. So instead of indulgence,',\n",
       "   'tokens': [307,\n",
       "    364,\n",
       "    19484,\n",
       "    27194,\n",
       "    318,\n",
       "    293,\n",
       "    611,\n",
       "    322,\n",
       "    264,\n",
       "    28626,\n",
       "    15260,\n",
       "    295,\n",
       "    19329,\n",
       "    3701,\n",
       "    13,\n",
       "    407,\n",
       "    2602,\n",
       "    295,\n",
       "    28626,\n",
       "    15260,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14698275397805607,\n",
       "   'compression_ratio': 1.8067632850241546,\n",
       "   'no_speech_prob': 6.176982424221933e-05},\n",
       "  {'id': 76,\n",
       "   'seek': 45172,\n",
       "   'start': 469.72,\n",
       "   'end': 475.32000000000005,\n",
       "   'text': ' it says condense. So these words where there is either the noise or when the voice drops,',\n",
       "   'tokens': [309,\n",
       "    1619,\n",
       "    2224,\n",
       "    1288,\n",
       "    13,\n",
       "    407,\n",
       "    613,\n",
       "    2283,\n",
       "    689,\n",
       "    456,\n",
       "    307,\n",
       "    2139,\n",
       "    264,\n",
       "    5658,\n",
       "    420,\n",
       "    562,\n",
       "    264,\n",
       "    3177,\n",
       "    11438,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.14698275397805607,\n",
       "   'compression_ratio': 1.8067632850241546,\n",
       "   'no_speech_prob': 6.176982424221933e-05},\n",
       "  {'id': 77,\n",
       "   'seek': 47532,\n",
       "   'start': 475.32,\n",
       "   'end': 482.59999999999997,\n",
       "   'text': ' the volume goes down. The speaker is not quite clear. You can see how the smaller size model performs',\n",
       "   'tokens': [264,\n",
       "    5523,\n",
       "    1709,\n",
       "    760,\n",
       "    13,\n",
       "    440,\n",
       "    8145,\n",
       "    307,\n",
       "    406,\n",
       "    1596,\n",
       "    1850,\n",
       "    13,\n",
       "    509,\n",
       "    393,\n",
       "    536,\n",
       "    577,\n",
       "    264,\n",
       "    4356,\n",
       "    2744,\n",
       "    2316,\n",
       "    26213],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12167131057893386,\n",
       "   'compression_ratio': 1.841860465116279,\n",
       "   'no_speech_prob': 0.00029646194889210165},\n",
       "  {'id': 78,\n",
       "   'seek': 47532,\n",
       "   'start': 483.4,\n",
       "   'end': 489.96,\n",
       "   'text': ' versus the larger size model. Let us take another example. So I am going to go pick another example',\n",
       "   'tokens': [5717,\n",
       "    264,\n",
       "    4833,\n",
       "    2744,\n",
       "    2316,\n",
       "    13,\n",
       "    961,\n",
       "    505,\n",
       "    747,\n",
       "    1071,\n",
       "    1365,\n",
       "    13,\n",
       "    407,\n",
       "    286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    352,\n",
       "    1888,\n",
       "    1071,\n",
       "    1365],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12167131057893386,\n",
       "   'compression_ratio': 1.841860465116279,\n",
       "   'no_speech_prob': 0.00029646194889210165},\n",
       "  {'id': 79,\n",
       "   'seek': 47532,\n",
       "   'start': 490.52,\n",
       "   'end': 497.96,\n",
       "   'text': ' from the same movie. So you look very fashionable apart from the mud. I am going to copy the path.',\n",
       "   'tokens': [490,\n",
       "    264,\n",
       "    912,\n",
       "    3169,\n",
       "    13,\n",
       "    407,\n",
       "    291,\n",
       "    574,\n",
       "    588,\n",
       "    40735,\n",
       "    4936,\n",
       "    490,\n",
       "    264,\n",
       "    8933,\n",
       "    13,\n",
       "    286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    5055,\n",
       "    264,\n",
       "    3100,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12167131057893386,\n",
       "   'compression_ratio': 1.841860465116279,\n",
       "   'no_speech_prob': 0.00029646194889210165},\n",
       "  {'id': 80,\n",
       "   'seek': 47532,\n",
       "   'start': 497.96,\n",
       "   'end': 503.0,\n",
       "   'text': ' I am going to copy the path here. Copy the audio path. Come back here and then I am going to do',\n",
       "   'tokens': [286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    5055,\n",
       "    264,\n",
       "    3100,\n",
       "    510,\n",
       "    13,\n",
       "    25653,\n",
       "    264,\n",
       "    6278,\n",
       "    3100,\n",
       "    13,\n",
       "    2492,\n",
       "    646,\n",
       "    510,\n",
       "    293,\n",
       "    550,\n",
       "    286,\n",
       "    669,\n",
       "    516,\n",
       "    281,\n",
       "    360],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12167131057893386,\n",
       "   'compression_ratio': 1.841860465116279,\n",
       "   'no_speech_prob': 0.00029646194889210165},\n",
       "  {'id': 81,\n",
       "   'seek': 50300,\n",
       "   'start': 503.0,\n",
       "   'end': 511.64,\n",
       "   'text': ' the same thing that I just did last time. Oops. Yep. Mud.mp3. When we downloaded, we are going to save',\n",
       "   'tokens': [264,\n",
       "    912,\n",
       "    551,\n",
       "    300,\n",
       "    286,\n",
       "    445,\n",
       "    630,\n",
       "    1036,\n",
       "    565,\n",
       "    13,\n",
       "    21726,\n",
       "    13,\n",
       "    7010,\n",
       "    13,\n",
       "    39231,\n",
       "    13,\n",
       "    2455,\n",
       "    18,\n",
       "    13,\n",
       "    1133,\n",
       "    321,\n",
       "    21748,\n",
       "    11,\n",
       "    321,\n",
       "    366,\n",
       "    516,\n",
       "    281,\n",
       "    3155],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15986149810081304,\n",
       "   'compression_ratio': 1.5228426395939085,\n",
       "   'no_speech_prob': 0.0005399673827923834},\n",
       "  {'id': 82,\n",
       "   'seek': 50300,\n",
       "   'start': 511.64,\n",
       "   'end': 522.68,\n",
       "   'text': \" it still in the same file name audio.mp3. It's played. Oops. Did it not download? Let me delete this\",\n",
       "   'tokens': [309,\n",
       "    920,\n",
       "    294,\n",
       "    264,\n",
       "    912,\n",
       "    3991,\n",
       "    1315,\n",
       "    6278,\n",
       "    13,\n",
       "    2455,\n",
       "    18,\n",
       "    13,\n",
       "    467,\n",
       "    311,\n",
       "    3737,\n",
       "    13,\n",
       "    21726,\n",
       "    13,\n",
       "    2589,\n",
       "    309,\n",
       "    406,\n",
       "    5484,\n",
       "    30,\n",
       "    961,\n",
       "    385,\n",
       "    12097,\n",
       "    341],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15986149810081304,\n",
       "   'compression_ratio': 1.5228426395939085,\n",
       "   'no_speech_prob': 0.0005399673827923834},\n",
       "  {'id': 83,\n",
       "   'seek': 50300,\n",
       "   'start': 522.68,\n",
       "   'end': 531.64,\n",
       "   'text': ' file just to make sure that this is gone. Come back here. Run this once again. Just to make sure',\n",
       "   'tokens': [3991,\n",
       "    445,\n",
       "    281,\n",
       "    652,\n",
       "    988,\n",
       "    300,\n",
       "    341,\n",
       "    307,\n",
       "    2780,\n",
       "    13,\n",
       "    2492,\n",
       "    646,\n",
       "    510,\n",
       "    13,\n",
       "    8950,\n",
       "    341,\n",
       "    1564,\n",
       "    797,\n",
       "    13,\n",
       "    1449,\n",
       "    281,\n",
       "    652,\n",
       "    988],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15986149810081304,\n",
       "   'compression_ratio': 1.5228426395939085,\n",
       "   'no_speech_prob': 0.0005399673827923834},\n",
       "  {'id': 84,\n",
       "   'seek': 53164,\n",
       "   'start': 531.64,\n",
       "   'end': 537.72,\n",
       "   'text': ' that we are downloading the right file and played. You look very fashionable.',\n",
       "   'tokens': [300,\n",
       "    321,\n",
       "    366,\n",
       "    32529,\n",
       "    264,\n",
       "    558,\n",
       "    3991,\n",
       "    293,\n",
       "    3737,\n",
       "    13,\n",
       "    509,\n",
       "    574,\n",
       "    588,\n",
       "    40735,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.17023020777209052,\n",
       "   'compression_ratio': 1.6071428571428572,\n",
       "   'no_speech_prob': 0.00025547249242663383},\n",
       "  {'id': 85,\n",
       "   'seek': 53164,\n",
       "   'start': 540.76,\n",
       "   'end': 546.12,\n",
       "   'text': ' And you can see the difference. Right? The first person who spoke spoken American accent.',\n",
       "   'tokens': [400,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    264,\n",
       "    2649,\n",
       "    13,\n",
       "    1779,\n",
       "    30,\n",
       "    440,\n",
       "    700,\n",
       "    954,\n",
       "    567,\n",
       "    7179,\n",
       "    10759,\n",
       "    2665,\n",
       "    11982,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.17023020777209052,\n",
       "   'compression_ratio': 1.6071428571428572,\n",
       "   'no_speech_prob': 0.00025547249242663383},\n",
       "  {'id': 86,\n",
       "   'seek': 53164,\n",
       "   'start': 546.12,\n",
       "   'end': 552.84,\n",
       "   'text': ' Now the person who spoke the spoken British accent. And this is Michael Kane. If you do not know.',\n",
       "   'tokens': [823,\n",
       "    264,\n",
       "    954,\n",
       "    567,\n",
       "    7179,\n",
       "    264,\n",
       "    10759,\n",
       "    6221,\n",
       "    11982,\n",
       "    13,\n",
       "    400,\n",
       "    341,\n",
       "    307,\n",
       "    5116,\n",
       "    39161,\n",
       "    13,\n",
       "    759,\n",
       "    291,\n",
       "    360,\n",
       "    406,\n",
       "    458,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.17023020777209052,\n",
       "   'compression_ratio': 1.6071428571428572,\n",
       "   'no_speech_prob': 0.00025547249242663383},\n",
       "  {'id': 87,\n",
       "   'seek': 53164,\n",
       "   'start': 552.84,\n",
       "   'end': 557.8,\n",
       "   'text': ' So now we are going to do transcription for that. First, we are going to use the larger model.',\n",
       "   'tokens': [407,\n",
       "    586,\n",
       "    321,\n",
       "    366,\n",
       "    516,\n",
       "    281,\n",
       "    360,\n",
       "    35288,\n",
       "    337,\n",
       "    300,\n",
       "    13,\n",
       "    2386,\n",
       "    11,\n",
       "    321,\n",
       "    366,\n",
       "    516,\n",
       "    281,\n",
       "    764,\n",
       "    264,\n",
       "    4833,\n",
       "    2316,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.17023020777209052,\n",
       "   'compression_ratio': 1.6071428571428572,\n",
       "   'no_speech_prob': 0.00025547249242663383},\n",
       "  {'id': 88,\n",
       "   'seek': 55780,\n",
       "   'start': 557.8,\n",
       "   'end': 565.7199999999999,\n",
       "   'text': ' In this case, we are going to use the model that is medium model. So it says you look very',\n",
       "   'tokens': [682,\n",
       "    341,\n",
       "    1389,\n",
       "    11,\n",
       "    321,\n",
       "    366,\n",
       "    516,\n",
       "    281,\n",
       "    764,\n",
       "    264,\n",
       "    2316,\n",
       "    300,\n",
       "    307,\n",
       "    6399,\n",
       "    2316,\n",
       "    13,\n",
       "    407,\n",
       "    309,\n",
       "    1619,\n",
       "    291,\n",
       "    574,\n",
       "    588],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13279797106373067,\n",
       "   'compression_ratio': 1.8357487922705313,\n",
       "   'no_speech_prob': 0.000195498185348697},\n",
       "  {'id': 89,\n",
       "   'seek': 55780,\n",
       "   'start': 565.7199999999999,\n",
       "   'end': 570.4399999999999,\n",
       "   'text': ' fashionable apart from the mud. I really suppose to be apart from the mud, but it says apart from',\n",
       "   'tokens': [40735,\n",
       "    4936,\n",
       "    490,\n",
       "    264,\n",
       "    8933,\n",
       "    13,\n",
       "    286,\n",
       "    534,\n",
       "    7297,\n",
       "    281,\n",
       "    312,\n",
       "    4936,\n",
       "    490,\n",
       "    264,\n",
       "    8933,\n",
       "    11,\n",
       "    457,\n",
       "    309,\n",
       "    1619,\n",
       "    4936,\n",
       "    490],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13279797106373067,\n",
       "   'compression_ratio': 1.8357487922705313,\n",
       "   'no_speech_prob': 0.000195498185348697},\n",
       "  {'id': 90,\n",
       "   'seek': 55780,\n",
       "   'start': 570.4399999999999,\n",
       "   'end': 577.8,\n",
       "   'text': \" the mud. Let's see what a tiny model does. It says you look very fashionable. Oh, pretty much.\",\n",
       "   'tokens': [264,\n",
       "    8933,\n",
       "    13,\n",
       "    961,\n",
       "    311,\n",
       "    536,\n",
       "    437,\n",
       "    257,\n",
       "    5870,\n",
       "    2316,\n",
       "    775,\n",
       "    13,\n",
       "    467,\n",
       "    1619,\n",
       "    291,\n",
       "    574,\n",
       "    588,\n",
       "    40735,\n",
       "    13,\n",
       "    876,\n",
       "    11,\n",
       "    1238,\n",
       "    709,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13279797106373067,\n",
       "   'compression_ratio': 1.8357487922705313,\n",
       "   'no_speech_prob': 0.000195498185348697},\n",
       "  {'id': 91,\n",
       "   'seek': 55780,\n",
       "   'start': 577.8,\n",
       "   'end': 584.3599999999999,\n",
       "   'text': ' So you can see the last word was like the only one letter was the mistake in the previous model.',\n",
       "   'tokens': [407,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    264,\n",
       "    1036,\n",
       "    1349,\n",
       "    390,\n",
       "    411,\n",
       "    264,\n",
       "    787,\n",
       "    472,\n",
       "    5063,\n",
       "    390,\n",
       "    264,\n",
       "    6146,\n",
       "    294,\n",
       "    264,\n",
       "    3894,\n",
       "    2316,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13279797106373067,\n",
       "   'compression_ratio': 1.8357487922705313,\n",
       "   'no_speech_prob': 0.000195498185348697},\n",
       "  {'id': 92,\n",
       "   'seek': 58436,\n",
       "   'start': 584.36,\n",
       "   'end': 589.72,\n",
       "   'text': ' But here you can see. Oh, pretty much. So you can see the huge difference. I would like to do one',\n",
       "   'tokens': [583,\n",
       "    510,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    13,\n",
       "    876,\n",
       "    11,\n",
       "    1238,\n",
       "    709,\n",
       "    13,\n",
       "    407,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    264,\n",
       "    2603,\n",
       "    2649,\n",
       "    13,\n",
       "    286,\n",
       "    576,\n",
       "    411,\n",
       "    281,\n",
       "    360,\n",
       "    472],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15656562473462976,\n",
       "   'compression_ratio': 1.5797665369649805,\n",
       "   'no_speech_prob': 3.3548654755577445e-05},\n",
       "  {'id': 93,\n",
       "   'seek': 58436,\n",
       "   'start': 589.72,\n",
       "   'end': 597.5600000000001,\n",
       "   'text': ' final test before we close the video. So this is the one. Bats are nocturnal. Bats might be,',\n",
       "   'tokens': [2572,\n",
       "    1500,\n",
       "    949,\n",
       "    321,\n",
       "    1998,\n",
       "    264,\n",
       "    960,\n",
       "    13,\n",
       "    407,\n",
       "    341,\n",
       "    307,\n",
       "    264,\n",
       "    472,\n",
       "    13,\n",
       "    363,\n",
       "    1720,\n",
       "    366,\n",
       "    572,\n",
       "    349,\n",
       "    925,\n",
       "    304,\n",
       "    13,\n",
       "    363,\n",
       "    1720,\n",
       "    1062,\n",
       "    312,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15656562473462976,\n",
       "   'compression_ratio': 1.5797665369649805,\n",
       "   'no_speech_prob': 3.3548654755577445e-05},\n",
       "  {'id': 94,\n",
       "   'seek': 58436,\n",
       "   'start': 597.5600000000001,\n",
       "   'end': 602.36,\n",
       "   'text': \" but even for billion playboys, three o'clock is pushing it. So I'm going to copy this\",\n",
       "   'tokens': [457,\n",
       "    754,\n",
       "    337,\n",
       "    5218,\n",
       "    862,\n",
       "    31638,\n",
       "    11,\n",
       "    1045,\n",
       "    277,\n",
       "    6,\n",
       "    9023,\n",
       "    307,\n",
       "    7380,\n",
       "    309,\n",
       "    13,\n",
       "    407,\n",
       "    286,\n",
       "    478,\n",
       "    516,\n",
       "    281,\n",
       "    5055,\n",
       "    341],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15656562473462976,\n",
       "   'compression_ratio': 1.5797665369649805,\n",
       "   'no_speech_prob': 3.3548654755577445e-05},\n",
       "  {'id': 95,\n",
       "   'seek': 58436,\n",
       "   'start': 603.4,\n",
       "   'end': 609.72,\n",
       "   'text': ' and copy the audio address. Come back to the Google collab. First, delete the file in case,',\n",
       "   'tokens': [293,\n",
       "    5055,\n",
       "    264,\n",
       "    6278,\n",
       "    2985,\n",
       "    13,\n",
       "    2492,\n",
       "    646,\n",
       "    281,\n",
       "    264,\n",
       "    3329,\n",
       "    44228,\n",
       "    13,\n",
       "    2386,\n",
       "    11,\n",
       "    12097,\n",
       "    264,\n",
       "    3991,\n",
       "    294,\n",
       "    1389,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.15656562473462976,\n",
       "   'compression_ratio': 1.5797665369649805,\n",
       "   'no_speech_prob': 3.3548654755577445e-05},\n",
       "  {'id': 96,\n",
       "   'seek': 60972,\n",
       "   'start': 609.72,\n",
       "   'end': 619.1600000000001,\n",
       "   'text': ' you know, if it is going to create any problem. Come back here, paste it, run this. So this is',\n",
       "   'tokens': [291,\n",
       "    458,\n",
       "    11,\n",
       "    498,\n",
       "    309,\n",
       "    307,\n",
       "    516,\n",
       "    281,\n",
       "    1884,\n",
       "    604,\n",
       "    1154,\n",
       "    13,\n",
       "    2492,\n",
       "    646,\n",
       "    510,\n",
       "    11,\n",
       "    9163,\n",
       "    309,\n",
       "    11,\n",
       "    1190,\n",
       "    341,\n",
       "    13,\n",
       "    407,\n",
       "    341,\n",
       "    307],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.174893758068346,\n",
       "   'compression_ratio': 1.4709302325581395,\n",
       "   'no_speech_prob': 5.750605487264693e-05},\n",
       "  {'id': 97,\n",
       "   'seek': 60972,\n",
       "   'start': 619.1600000000001,\n",
       "   'end': 625.96,\n",
       "   'text': \" going to download the file after it downloads. I'm going to play it. I think I have to run it.\",\n",
       "   'tokens': [516,\n",
       "    281,\n",
       "    5484,\n",
       "    264,\n",
       "    3991,\n",
       "    934,\n",
       "    309,\n",
       "    36553,\n",
       "    13,\n",
       "    286,\n",
       "    478,\n",
       "    516,\n",
       "    281,\n",
       "    862,\n",
       "    309,\n",
       "    13,\n",
       "    286,\n",
       "    519,\n",
       "    286,\n",
       "    362,\n",
       "    281,\n",
       "    1190,\n",
       "    309,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.174893758068346,\n",
       "   'compression_ratio': 1.4709302325581395,\n",
       "   'no_speech_prob': 5.750605487264693e-05},\n",
       "  {'id': 98,\n",
       "   'seek': 62596,\n",
       "   'start': 625.96,\n",
       "   'end': 641.8000000000001,\n",
       "   'text': \" I'm sorry. That's my mistake. Okay. Let's do the transcription. Bats are not pernil instead\",\n",
       "   'tokens': [286,\n",
       "    478,\n",
       "    2597,\n",
       "    13,\n",
       "    663,\n",
       "    311,\n",
       "    452,\n",
       "    6146,\n",
       "    13,\n",
       "    1033,\n",
       "    13,\n",
       "    961,\n",
       "    311,\n",
       "    360,\n",
       "    264,\n",
       "    35288,\n",
       "    13,\n",
       "    363,\n",
       "    1720,\n",
       "    366,\n",
       "    406,\n",
       "    680,\n",
       "    77,\n",
       "    388,\n",
       "    2602],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2142820473176887,\n",
       "   'compression_ratio': 1.53551912568306,\n",
       "   'no_speech_prob': 8.890536992112175e-05},\n",
       "  {'id': 99,\n",
       "   'seek': 62596,\n",
       "   'start': 641.8000000000001,\n",
       "   'end': 646.2,\n",
       "   'text': \" of saying nocturnal. Bats maybe, but even a billionaires playboy three o'clock is pushing him.\",\n",
       "   'tokens': [295,\n",
       "    1566,\n",
       "    572,\n",
       "    349,\n",
       "    925,\n",
       "    304,\n",
       "    13,\n",
       "    363,\n",
       "    1720,\n",
       "    1310,\n",
       "    11,\n",
       "    457,\n",
       "    754,\n",
       "    257,\n",
       "    5218,\n",
       "    10840,\n",
       "    862,\n",
       "    12795,\n",
       "    1045,\n",
       "    277,\n",
       "    6,\n",
       "    9023,\n",
       "    307,\n",
       "    7380,\n",
       "    796,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2142820473176887,\n",
       "   'compression_ratio': 1.53551912568306,\n",
       "   'no_speech_prob': 8.890536992112175e-05},\n",
       "  {'id': 100,\n",
       "   'seek': 62596,\n",
       "   'start': 647.32,\n",
       "   'end': 655.24,\n",
       "   'text': \" Let's run this. English. That's your knock journal. That's my bite. But even for a billionaire\",\n",
       "   'tokens': [961,\n",
       "    311,\n",
       "    1190,\n",
       "    341,\n",
       "    13,\n",
       "    3669,\n",
       "    13,\n",
       "    663,\n",
       "    311,\n",
       "    428,\n",
       "    6728,\n",
       "    6708,\n",
       "    13,\n",
       "    663,\n",
       "    311,\n",
       "    452,\n",
       "    7988,\n",
       "    13,\n",
       "    583,\n",
       "    754,\n",
       "    337,\n",
       "    257,\n",
       "    5218,\n",
       "    9020],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.2142820473176887,\n",
       "   'compression_ratio': 1.53551912568306,\n",
       "   'no_speech_prob': 8.890536992112175e-05},\n",
       "  {'id': 101,\n",
       "   'seek': 65524,\n",
       "   'start': 655.24,\n",
       "   'end': 659.88,\n",
       "   'text': ' playboy where the clock is pushing me, you can see the difference in the quality of transcription',\n",
       "   'tokens': [862,\n",
       "    12795,\n",
       "    689,\n",
       "    264,\n",
       "    7830,\n",
       "    307,\n",
       "    7380,\n",
       "    385,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    264,\n",
       "    2649,\n",
       "    294,\n",
       "    264,\n",
       "    3125,\n",
       "    295,\n",
       "    35288],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12049112215146913,\n",
       "   'compression_ratio': 1.8240740740740742,\n",
       "   'no_speech_prob': 0.0003374592633917928},\n",
       "  {'id': 102,\n",
       "   'seek': 65524,\n",
       "   'start': 659.88,\n",
       "   'end': 667.48,\n",
       "   'text': ' even for English when you change different models. But the point here is not to pick on the models,',\n",
       "   'tokens': [754,\n",
       "    337,\n",
       "    3669,\n",
       "    562,\n",
       "    291,\n",
       "    1319,\n",
       "    819,\n",
       "    5245,\n",
       "    13,\n",
       "    583,\n",
       "    264,\n",
       "    935,\n",
       "    510,\n",
       "    307,\n",
       "    406,\n",
       "    281,\n",
       "    1888,\n",
       "    322,\n",
       "    264,\n",
       "    5245,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12049112215146913,\n",
       "   'compression_ratio': 1.8240740740740742,\n",
       "   'no_speech_prob': 0.0003374592633917928},\n",
       "  {'id': 103,\n",
       "   'seek': 65524,\n",
       "   'start': 667.48,\n",
       "   'end': 674.36,\n",
       "   'text': ' but the point here is you always have to make this call the trade of between the size of the model,',\n",
       "   'tokens': [457,\n",
       "    264,\n",
       "    935,\n",
       "    510,\n",
       "    307,\n",
       "    291,\n",
       "    1009,\n",
       "    362,\n",
       "    281,\n",
       "    652,\n",
       "    341,\n",
       "    818,\n",
       "    264,\n",
       "    4923,\n",
       "    295,\n",
       "    1296,\n",
       "    264,\n",
       "    2744,\n",
       "    295,\n",
       "    264,\n",
       "    2316,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12049112215146913,\n",
       "   'compression_ratio': 1.8240740740740742,\n",
       "   'no_speech_prob': 0.0003374592633917928},\n",
       "  {'id': 104,\n",
       "   'seek': 65524,\n",
       "   'start': 674.36,\n",
       "   'end': 680.2,\n",
       "   'text': \" the performance in when I say performance, the speed of the model's inference and also the error\",\n",
       "   'tokens': [264,\n",
       "    3389,\n",
       "    294,\n",
       "    562,\n",
       "    286,\n",
       "    584,\n",
       "    3389,\n",
       "    11,\n",
       "    264,\n",
       "    3073,\n",
       "    295,\n",
       "    264,\n",
       "    2316,\n",
       "    311,\n",
       "    38253,\n",
       "    293,\n",
       "    611,\n",
       "    264,\n",
       "    6713],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.12049112215146913,\n",
       "   'compression_ratio': 1.8240740740740742,\n",
       "   'no_speech_prob': 0.0003374592633917928},\n",
       "  {'id': 105,\n",
       "   'seek': 68020,\n",
       "   'start': 680.2,\n",
       "   'end': 685.96,\n",
       "   'text': \" rate. So it's a call that you have to make, but it's my duty for me to tell you how the tiny\",\n",
       "   'tokens': [3314,\n",
       "    13,\n",
       "    407,\n",
       "    309,\n",
       "    311,\n",
       "    257,\n",
       "    818,\n",
       "    300,\n",
       "    291,\n",
       "    362,\n",
       "    281,\n",
       "    652,\n",
       "    11,\n",
       "    457,\n",
       "    309,\n",
       "    311,\n",
       "    452,\n",
       "    9776,\n",
       "    337,\n",
       "    385,\n",
       "    281,\n",
       "    980,\n",
       "    291,\n",
       "    577,\n",
       "    264,\n",
       "    5870],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10582417189472854,\n",
       "   'compression_ratio': 1.7387387387387387,\n",
       "   'no_speech_prob': 0.00028838825528509915},\n",
       "  {'id': 106,\n",
       "   'seek': 68020,\n",
       "   'start': 685.96,\n",
       "   'end': 691.48,\n",
       "   'text': ' model is performing versus how the large model is performing for the same audio clip. But if you',\n",
       "   'tokens': [2316,\n",
       "    307,\n",
       "    10205,\n",
       "    5717,\n",
       "    577,\n",
       "    264,\n",
       "    2416,\n",
       "    2316,\n",
       "    307,\n",
       "    10205,\n",
       "    337,\n",
       "    264,\n",
       "    912,\n",
       "    6278,\n",
       "    7353,\n",
       "    13,\n",
       "    583,\n",
       "    498,\n",
       "    291],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10582417189472854,\n",
       "   'compression_ratio': 1.7387387387387387,\n",
       "   'no_speech_prob': 0.00028838825528509915},\n",
       "  {'id': 107,\n",
       "   'seek': 68020,\n",
       "   'start': 691.48,\n",
       "   'end': 699.8000000000001,\n",
       "   'text': ' just consider the medium model, which is not the largest model, this is the 769, 769 million',\n",
       "   'tokens': [445,\n",
       "    1949,\n",
       "    264,\n",
       "    6399,\n",
       "    2316,\n",
       "    11,\n",
       "    597,\n",
       "    307,\n",
       "    406,\n",
       "    264,\n",
       "    6443,\n",
       "    2316,\n",
       "    11,\n",
       "    341,\n",
       "    307,\n",
       "    264,\n",
       "    1614,\n",
       "    30908,\n",
       "    11,\n",
       "    1614,\n",
       "    30908,\n",
       "    2459],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10582417189472854,\n",
       "   'compression_ratio': 1.7387387387387387,\n",
       "   'no_speech_prob': 0.00028838825528509915},\n",
       "  {'id': 108,\n",
       "   'seek': 68020,\n",
       "   'start': 699.8000000000001,\n",
       "   'end': 708.2800000000001,\n",
       "   'text': ' parameter model. But even this model is quite amazing. It is not just simply amazing for the US accent,',\n",
       "   'tokens': [13075,\n",
       "    2316,\n",
       "    13,\n",
       "    583,\n",
       "    754,\n",
       "    341,\n",
       "    2316,\n",
       "    307,\n",
       "    1596,\n",
       "    2243,\n",
       "    13,\n",
       "    467,\n",
       "    307,\n",
       "    406,\n",
       "    445,\n",
       "    2935,\n",
       "    2243,\n",
       "    337,\n",
       "    264,\n",
       "    2546,\n",
       "    11982,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.10582417189472854,\n",
       "   'compression_ratio': 1.7387387387387387,\n",
       "   'no_speech_prob': 0.00028838825528509915},\n",
       "  {'id': 109,\n",
       "   'seek': 70828,\n",
       "   'start': 708.28,\n",
       "   'end': 713.8,\n",
       "   'text': ' but you also saw that it was doing a good job for the UK accent, which is a British accent.',\n",
       "   'tokens': [457,\n",
       "    291,\n",
       "    611,\n",
       "    1866,\n",
       "    300,\n",
       "    309,\n",
       "    390,\n",
       "    884,\n",
       "    257,\n",
       "    665,\n",
       "    1691,\n",
       "    337,\n",
       "    264,\n",
       "    7051,\n",
       "    11982,\n",
       "    11,\n",
       "    597,\n",
       "    307,\n",
       "    257,\n",
       "    6221,\n",
       "    11982,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16418802849600247,\n",
       "   'compression_ratio': 1.7913385826771653,\n",
       "   'no_speech_prob': 0.0006739023374393582},\n",
       "  {'id': 110,\n",
       "   'seek': 70828,\n",
       "   'start': 713.8,\n",
       "   'end': 719.9599999999999,\n",
       "   'text': ' And I myself have tested Indian accent with that. And then it has done a really good job',\n",
       "   'tokens': [400,\n",
       "    286,\n",
       "    2059,\n",
       "    362,\n",
       "    8246,\n",
       "    6427,\n",
       "    11982,\n",
       "    365,\n",
       "    300,\n",
       "    13,\n",
       "    400,\n",
       "    550,\n",
       "    309,\n",
       "    575,\n",
       "    1096,\n",
       "    257,\n",
       "    534,\n",
       "    665,\n",
       "    1691],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16418802849600247,\n",
       "   'compression_ratio': 1.7913385826771653,\n",
       "   'no_speech_prob': 0.0006739023374393582},\n",
       "  {'id': 111,\n",
       "   'seek': 70828,\n",
       "   'start': 719.9599999999999,\n",
       "   'end': 725.48,\n",
       "   'text': ' even for Indian accent English, which is not something that I usually see with Google',\n",
       "   'tokens': [754,\n",
       "    337,\n",
       "    6427,\n",
       "    11982,\n",
       "    3669,\n",
       "    11,\n",
       "    597,\n",
       "    307,\n",
       "    406,\n",
       "    746,\n",
       "    300,\n",
       "    286,\n",
       "    2673,\n",
       "    536,\n",
       "    365,\n",
       "    3329],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16418802849600247,\n",
       "   'compression_ratio': 1.7913385826771653,\n",
       "   'no_speech_prob': 0.0006739023374393582},\n",
       "  {'id': 112,\n",
       "   'seek': 70828,\n",
       "   'start': 725.48,\n",
       "   'end': 731.3199999999999,\n",
       "   'text': ' Assistant or Alexa or Siri. They usually struggle with Indian accent until, you know,',\n",
       "   'tokens': [14890,\n",
       "    420,\n",
       "    22595,\n",
       "    420,\n",
       "    33682,\n",
       "    13,\n",
       "    814,\n",
       "    2673,\n",
       "    7799,\n",
       "    365,\n",
       "    6427,\n",
       "    11982,\n",
       "    1826,\n",
       "    11,\n",
       "    291,\n",
       "    458,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16418802849600247,\n",
       "   'compression_ratio': 1.7913385826771653,\n",
       "   'no_speech_prob': 0.0006739023374393582},\n",
       "  {'id': 113,\n",
       "   'seek': 70828,\n",
       "   'start': 731.3199999999999,\n",
       "   'end': 737.24,\n",
       "   'text': \" it's really customized for Indian market. But otherwise, I usually see them struggling. But even then,\",\n",
       "   'tokens': [309,\n",
       "    311,\n",
       "    534,\n",
       "    30581,\n",
       "    337,\n",
       "    6427,\n",
       "    2142,\n",
       "    13,\n",
       "    583,\n",
       "    5911,\n",
       "    11,\n",
       "    286,\n",
       "    2673,\n",
       "    536,\n",
       "    552,\n",
       "    9314,\n",
       "    13,\n",
       "    583,\n",
       "    754,\n",
       "    550,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.16418802849600247,\n",
       "   'compression_ratio': 1.7913385826771653,\n",
       "   'no_speech_prob': 0.0006739023374393582},\n",
       "  {'id': 114,\n",
       "   'seek': 73724,\n",
       "   'start': 737.24,\n",
       "   'end': 742.76,\n",
       "   'text': ' this complete, like the simple open source model, open AI whisper is doing really a great job.',\n",
       "   'tokens': [341,\n",
       "    3566,\n",
       "    11,\n",
       "    411,\n",
       "    264,\n",
       "    2199,\n",
       "    1269,\n",
       "    4009,\n",
       "    2316,\n",
       "    11,\n",
       "    1269,\n",
       "    7318,\n",
       "    26018,\n",
       "    307,\n",
       "    884,\n",
       "    534,\n",
       "    257,\n",
       "    869,\n",
       "    1691,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1646775950556216,\n",
       "   'compression_ratio': 1.6992753623188406,\n",
       "   'no_speech_prob': 0.0004852503479924053},\n",
       "  {'id': 115,\n",
       "   'seek': 73724,\n",
       "   'start': 742.76,\n",
       "   'end': 750.04,\n",
       "   'text': ' So in 2022, if you want to build a speech to text solution using Python, I think you should turn',\n",
       "   'tokens': [407,\n",
       "    294,\n",
       "    945,\n",
       "    7490,\n",
       "    11,\n",
       "    498,\n",
       "    291,\n",
       "    528,\n",
       "    281,\n",
       "    1322,\n",
       "    257,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    3827,\n",
       "    1228,\n",
       "    15329,\n",
       "    11,\n",
       "    286,\n",
       "    519,\n",
       "    291,\n",
       "    820,\n",
       "    1261],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1646775950556216,\n",
       "   'compression_ratio': 1.6992753623188406,\n",
       "   'no_speech_prob': 0.0004852503479924053},\n",
       "  {'id': 116,\n",
       "   'seek': 73724,\n",
       "   'start': 750.04,\n",
       "   'end': 756.04,\n",
       "   'text': \" towards opening a whisper. It's completely open source. You can see all the details about,\",\n",
       "   'tokens': [3030,\n",
       "    5193,\n",
       "    257,\n",
       "    26018,\n",
       "    13,\n",
       "    467,\n",
       "    311,\n",
       "    2584,\n",
       "    1269,\n",
       "    4009,\n",
       "    13,\n",
       "    509,\n",
       "    393,\n",
       "    536,\n",
       "    439,\n",
       "    264,\n",
       "    4365,\n",
       "    466,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1646775950556216,\n",
       "   'compression_ratio': 1.6992753623188406,\n",
       "   'no_speech_prob': 0.0004852503479924053},\n",
       "  {'id': 117,\n",
       "   'seek': 73724,\n",
       "   'start': 756.04,\n",
       "   'end': 759.8,\n",
       "   'text': ' you know, what is the error rate for, sorry, what is the error rate for each language?',\n",
       "   'tokens': [291,\n",
       "    458,\n",
       "    11,\n",
       "    437,\n",
       "    307,\n",
       "    264,\n",
       "    6713,\n",
       "    3314,\n",
       "    337,\n",
       "    11,\n",
       "    2597,\n",
       "    11,\n",
       "    437,\n",
       "    307,\n",
       "    264,\n",
       "    6713,\n",
       "    3314,\n",
       "    337,\n",
       "    1184,\n",
       "    2856,\n",
       "    30],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1646775950556216,\n",
       "   'compression_ratio': 1.6992753623188406,\n",
       "   'no_speech_prob': 0.0004852503479924053},\n",
       "  {'id': 118,\n",
       "   'seek': 73724,\n",
       "   'start': 759.8,\n",
       "   'end': 764.76,\n",
       "   'text': ' Word error rate, W here. What does the difference between each model and what other things that you',\n",
       "   'tokens': [8725,\n",
       "    6713,\n",
       "    3314,\n",
       "    11,\n",
       "    343,\n",
       "    510,\n",
       "    13,\n",
       "    708,\n",
       "    775,\n",
       "    264,\n",
       "    2649,\n",
       "    1296,\n",
       "    1184,\n",
       "    2316,\n",
       "    293,\n",
       "    437,\n",
       "    661,\n",
       "    721,\n",
       "    300,\n",
       "    291],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1646775950556216,\n",
       "   'compression_ratio': 1.6992753623188406,\n",
       "   'no_speech_prob': 0.0004852503479924053},\n",
       "  {'id': 119,\n",
       "   'seek': 76476,\n",
       "   'start': 764.76,\n",
       "   'end': 771.08,\n",
       "   'text': ' can do? For example, translation and multilingual translation and language detection and all those',\n",
       "   'tokens': [393,\n",
       "    360,\n",
       "    30,\n",
       "    1171,\n",
       "    1365,\n",
       "    11,\n",
       "    12853,\n",
       "    293,\n",
       "    2120,\n",
       "    38219,\n",
       "    12853,\n",
       "    293,\n",
       "    2856,\n",
       "    17784,\n",
       "    293,\n",
       "    439,\n",
       "    729],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13566997595000685,\n",
       "   'compression_ratio': 1.8257575757575757,\n",
       "   'no_speech_prob': 0.00039578418363817036},\n",
       "  {'id': 120,\n",
       "   'seek': 76476,\n",
       "   'start': 771.08,\n",
       "   'end': 776.76,\n",
       "   'text': \" things. And this, this particular project is aimed at somebody who doesn't know how to do speech\",\n",
       "   'tokens': [721,\n",
       "    13,\n",
       "    400,\n",
       "    341,\n",
       "    11,\n",
       "    341,\n",
       "    1729,\n",
       "    1716,\n",
       "    307,\n",
       "    20540,\n",
       "    412,\n",
       "    2618,\n",
       "    567,\n",
       "    1177,\n",
       "    380,\n",
       "    458,\n",
       "    577,\n",
       "    281,\n",
       "    360,\n",
       "    6218],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13566997595000685,\n",
       "   'compression_ratio': 1.8257575757575757,\n",
       "   'no_speech_prob': 0.00039578418363817036},\n",
       "  {'id': 121,\n",
       "   'seek': 76476,\n",
       "   'start': 776.76,\n",
       "   'end': 781.8,\n",
       "   'text': ' to text. And the point is, just literally like three lines of Python code, like if you literally',\n",
       "   'tokens': [281,\n",
       "    2487,\n",
       "    13,\n",
       "    400,\n",
       "    264,\n",
       "    935,\n",
       "    307,\n",
       "    11,\n",
       "    445,\n",
       "    3736,\n",
       "    411,\n",
       "    1045,\n",
       "    3876,\n",
       "    295,\n",
       "    15329,\n",
       "    3089,\n",
       "    11,\n",
       "    411,\n",
       "    498,\n",
       "    291,\n",
       "    3736],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13566997595000685,\n",
       "   'compression_ratio': 1.8257575757575757,\n",
       "   'no_speech_prob': 0.00039578418363817036},\n",
       "  {'id': 122,\n",
       "   'seek': 76476,\n",
       "   'start': 781.8,\n",
       "   'end': 786.92,\n",
       "   'text': ' see the code, you import, then you load the model and then you can just literally, you know,',\n",
       "   'tokens': [536,\n",
       "    264,\n",
       "    3089,\n",
       "    11,\n",
       "    291,\n",
       "    974,\n",
       "    11,\n",
       "    550,\n",
       "    291,\n",
       "    3677,\n",
       "    264,\n",
       "    2316,\n",
       "    293,\n",
       "    550,\n",
       "    291,\n",
       "    393,\n",
       "    445,\n",
       "    3736,\n",
       "    11,\n",
       "    291,\n",
       "    458,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13566997595000685,\n",
       "   'compression_ratio': 1.8257575757575757,\n",
       "   'no_speech_prob': 0.00039578418363817036},\n",
       "  {'id': 123,\n",
       "   'seek': 76476,\n",
       "   'start': 786.92,\n",
       "   'end': 791.96,\n",
       "   'text': ' transcribe and print the result. So basically in three lines of Python, let me run this for you.',\n",
       "   'tokens': [1145,\n",
       "    8056,\n",
       "    293,\n",
       "    4482,\n",
       "    264,\n",
       "    1874,\n",
       "    13,\n",
       "    407,\n",
       "    1936,\n",
       "    294,\n",
       "    1045,\n",
       "    3876,\n",
       "    295,\n",
       "    15329,\n",
       "    11,\n",
       "    718,\n",
       "    385,\n",
       "    1190,\n",
       "    341,\n",
       "    337,\n",
       "    291,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13566997595000685,\n",
       "   'compression_ratio': 1.8257575757575757,\n",
       "   'no_speech_prob': 0.00039578418363817036},\n",
       "  {'id': 124,\n",
       "   'seek': 79196,\n",
       "   'start': 791.96,\n",
       "   'end': 808.52,\n",
       "   'text': \" So I'm going to say three, sorry, three lines of Python code for speech to text. Okay. So what\",\n",
       "   'tokens': [407,\n",
       "    286,\n",
       "    478,\n",
       "    516,\n",
       "    281,\n",
       "    584,\n",
       "    1045,\n",
       "    11,\n",
       "    2597,\n",
       "    11,\n",
       "    1045,\n",
       "    3876,\n",
       "    295,\n",
       "    15329,\n",
       "    3089,\n",
       "    337,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    13,\n",
       "    1033,\n",
       "    13,\n",
       "    407,\n",
       "    437],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.17234807544284397,\n",
       "   'compression_ratio': 1.4172661870503598,\n",
       "   'no_speech_prob': 0.00019535934552550316},\n",
       "  {'id': 125,\n",
       "   'seek': 79196,\n",
       "   'start': 808.52,\n",
       "   'end': 819.88,\n",
       "   'text': \" am I going to do? I'm going to say import whisper cool. Then model is equal to whisper.load underscore\",\n",
       "   'tokens': [669,\n",
       "    286,\n",
       "    516,\n",
       "    281,\n",
       "    360,\n",
       "    30,\n",
       "    286,\n",
       "    478,\n",
       "    516,\n",
       "    281,\n",
       "    584,\n",
       "    974,\n",
       "    26018,\n",
       "    1627,\n",
       "    13,\n",
       "    1396,\n",
       "    2316,\n",
       "    307,\n",
       "    2681,\n",
       "    281,\n",
       "    26018,\n",
       "    13,\n",
       "    2907,\n",
       "    37556],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.17234807544284397,\n",
       "   'compression_ratio': 1.4172661870503598,\n",
       "   'no_speech_prob': 0.00019535934552550316},\n",
       "  {'id': 126,\n",
       "   'seek': 81988,\n",
       "   'start': 819.88,\n",
       "   'end': 833.48,\n",
       "   'text': \" model. And I can say medium here. And then model dot transcribe of audio dot MP3. And then while I'm\",\n",
       "   'tokens': [2316,\n",
       "    13,\n",
       "    400,\n",
       "    286,\n",
       "    393,\n",
       "    584,\n",
       "    6399,\n",
       "    510,\n",
       "    13,\n",
       "    400,\n",
       "    550,\n",
       "    2316,\n",
       "    5893,\n",
       "    1145,\n",
       "    8056,\n",
       "    295,\n",
       "    6278,\n",
       "    5893,\n",
       "    14146,\n",
       "    18,\n",
       "    13,\n",
       "    400,\n",
       "    550,\n",
       "    1339,\n",
       "    286,\n",
       "    478],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13512074075094083,\n",
       "   'compression_ratio': 1.6206896551724137,\n",
       "   'no_speech_prob': 0.00033931818325072527},\n",
       "  {'id': 127,\n",
       "   'seek': 81988,\n",
       "   'start': 833.48,\n",
       "   'end': 841.72,\n",
       "   'text': \" doing that, I can print the text. And when I run this, it should oops, there is load model. I'm\",\n",
       "   'tokens': [884,\n",
       "    300,\n",
       "    11,\n",
       "    286,\n",
       "    393,\n",
       "    4482,\n",
       "    264,\n",
       "    2487,\n",
       "    13,\n",
       "    400,\n",
       "    562,\n",
       "    286,\n",
       "    1190,\n",
       "    341,\n",
       "    11,\n",
       "    309,\n",
       "    820,\n",
       "    34166,\n",
       "    11,\n",
       "    456,\n",
       "    307,\n",
       "    3677,\n",
       "    2316,\n",
       "    13,\n",
       "    286,\n",
       "    478],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13512074075094083,\n",
       "   'compression_ratio': 1.6206896551724137,\n",
       "   'no_speech_prob': 0.00033931818325072527},\n",
       "  {'id': 128,\n",
       "   'seek': 81988,\n",
       "   'start': 841.72,\n",
       "   'end': 848.44,\n",
       "   'text': \" so sorry load model. And while I'm running this, you can see at the end of this step,\",\n",
       "   'tokens': [370,\n",
       "    2597,\n",
       "    3677,\n",
       "    2316,\n",
       "    13,\n",
       "    400,\n",
       "    1339,\n",
       "    286,\n",
       "    478,\n",
       "    2614,\n",
       "    341,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    536,\n",
       "    412,\n",
       "    264,\n",
       "    917,\n",
       "    295,\n",
       "    341,\n",
       "    1823,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.13512074075094083,\n",
       "   'compression_ratio': 1.6206896551724137,\n",
       "   'no_speech_prob': 0.00033931818325072527},\n",
       "  {'id': 129,\n",
       "   'seek': 84844,\n",
       "   'start': 848.44,\n",
       "   'end': 855.24,\n",
       "   'text': ' it is going to actually print the transcription of the audio. So in literally three lines of Python',\n",
       "   'tokens': [309,\n",
       "    307,\n",
       "    516,\n",
       "    281,\n",
       "    767,\n",
       "    4482,\n",
       "    264,\n",
       "    35288,\n",
       "    295,\n",
       "    264,\n",
       "    6278,\n",
       "    13,\n",
       "    407,\n",
       "    294,\n",
       "    3736,\n",
       "    1045,\n",
       "    3876,\n",
       "    295,\n",
       "    15329],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08634166927128048,\n",
       "   'compression_ratio': 1.7136563876651982,\n",
       "   'no_speech_prob': 0.000238408159930259},\n",
       "  {'id': 130,\n",
       "   'seek': 84844,\n",
       "   'start': 855.24,\n",
       "   'end': 863.08,\n",
       "   'text': ' code, you can have a cutting edge state of the art speech to text model, a speech to text project',\n",
       "   'tokens': [3089,\n",
       "    11,\n",
       "    291,\n",
       "    393,\n",
       "    362,\n",
       "    257,\n",
       "    6492,\n",
       "    4691,\n",
       "    1785,\n",
       "    295,\n",
       "    264,\n",
       "    1523,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    2316,\n",
       "    11,\n",
       "    257,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    1716],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08634166927128048,\n",
       "   'compression_ratio': 1.7136563876651982,\n",
       "   'no_speech_prob': 0.000238408159930259},\n",
       "  {'id': 131,\n",
       "   'seek': 84844,\n",
       "   'start': 863.08,\n",
       "   'end': 870.0400000000001,\n",
       "   'text': ' in Python. And that is quite quite quite amazing. And I think you should definitely check this out.',\n",
       "   'tokens': [294,\n",
       "    15329,\n",
       "    13,\n",
       "    400,\n",
       "    300,\n",
       "    307,\n",
       "    1596,\n",
       "    1596,\n",
       "    1596,\n",
       "    2243,\n",
       "    13,\n",
       "    400,\n",
       "    286,\n",
       "    519,\n",
       "    291,\n",
       "    820,\n",
       "    2138,\n",
       "    1520,\n",
       "    341,\n",
       "    484,\n",
       "    13],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08634166927128048,\n",
       "   'compression_ratio': 1.7136563876651982,\n",
       "   'no_speech_prob': 0.000238408159930259},\n",
       "  {'id': 132,\n",
       "   'seek': 84844,\n",
       "   'start': 870.0400000000001,\n",
       "   'end': 874.5200000000001,\n",
       "   'text': ' And if you are a university student trying to do something in speech to text, this is quite',\n",
       "   'tokens': [400,\n",
       "    498,\n",
       "    291,\n",
       "    366,\n",
       "    257,\n",
       "    5454,\n",
       "    3107,\n",
       "    1382,\n",
       "    281,\n",
       "    360,\n",
       "    746,\n",
       "    294,\n",
       "    6218,\n",
       "    281,\n",
       "    2487,\n",
       "    11,\n",
       "    341,\n",
       "    307,\n",
       "    1596],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.08634166927128048,\n",
       "   'compression_ratio': 1.7136563876651982,\n",
       "   'no_speech_prob': 0.000238408159930259},\n",
       "  {'id': 133,\n",
       "   'seek': 87452,\n",
       "   'start': 874.52,\n",
       "   'end': 879.4,\n",
       "   'text': \" multi-lingual. Even if you're not trying to do something for English, there are a lot, lot more\",\n",
       "   'tokens': [4825,\n",
       "    12,\n",
       "    1688,\n",
       "    901,\n",
       "    13,\n",
       "    2754,\n",
       "    498,\n",
       "    291,\n",
       "    434,\n",
       "    406,\n",
       "    1382,\n",
       "    281,\n",
       "    360,\n",
       "    746,\n",
       "    337,\n",
       "    3669,\n",
       "    11,\n",
       "    456,\n",
       "    366,\n",
       "    257,\n",
       "    688,\n",
       "    11,\n",
       "    688,\n",
       "    544],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1311329564740581,\n",
       "   'compression_ratio': 1.6593886462882097,\n",
       "   'no_speech_prob': 0.00019747338956221938},\n",
       "  {'id': 134,\n",
       "   'seek': 87452,\n",
       "   'start': 879.4,\n",
       "   'end': 884.4399999999999,\n",
       "   'text': ' languages. You can see all the languages here. So I come from India, all these Indian languages',\n",
       "   'tokens': [8650,\n",
       "    13,\n",
       "    509,\n",
       "    393,\n",
       "    536,\n",
       "    439,\n",
       "    264,\n",
       "    8650,\n",
       "    510,\n",
       "    13,\n",
       "    407,\n",
       "    286,\n",
       "    808,\n",
       "    490,\n",
       "    5282,\n",
       "    11,\n",
       "    439,\n",
       "    613,\n",
       "    6427,\n",
       "    8650],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1311329564740581,\n",
       "   'compression_ratio': 1.6593886462882097,\n",
       "   'no_speech_prob': 0.00019747338956221938},\n",
       "  {'id': 135,\n",
       "   'seek': 87452,\n",
       "   'start': 884.4399999999999,\n",
       "   'end': 890.76,\n",
       "   'text': ' are there, which is quite not not common with other other language models. So I would strongly',\n",
       "   'tokens': [366,\n",
       "    456,\n",
       "    11,\n",
       "    597,\n",
       "    307,\n",
       "    1596,\n",
       "    406,\n",
       "    406,\n",
       "    2689,\n",
       "    365,\n",
       "    661,\n",
       "    661,\n",
       "    2856,\n",
       "    5245,\n",
       "    13,\n",
       "    407,\n",
       "    286,\n",
       "    576,\n",
       "    10613],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1311329564740581,\n",
       "   'compression_ratio': 1.6593886462882097,\n",
       "   'no_speech_prob': 0.00019747338956221938},\n",
       "  {'id': 136,\n",
       "   'seek': 87452,\n",
       "   'start': 890.76,\n",
       "   'end': 895.72,\n",
       "   'text': ' recommend you to check this out. Any question, let me know in the comment section. Otherwise,',\n",
       "   'tokens': [2748,\n",
       "    291,\n",
       "    281,\n",
       "    1520,\n",
       "    341,\n",
       "    484,\n",
       "    13,\n",
       "    2639,\n",
       "    1168,\n",
       "    11,\n",
       "    718,\n",
       "    385,\n",
       "    458,\n",
       "    294,\n",
       "    264,\n",
       "    2871,\n",
       "    3541,\n",
       "    13,\n",
       "    10328,\n",
       "    11],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.1311329564740581,\n",
       "   'compression_ratio': 1.6593886462882097,\n",
       "   'no_speech_prob': 0.00019747338956221938},\n",
       "  {'id': 137,\n",
       "   'seek': 89572,\n",
       "   'start': 895.72,\n",
       "   'end': 905.72,\n",
       "   'text': ' see you in the next video. Happy coding.',\n",
       "   'tokens': [50364, 536, 291, 294, 264, 958, 960, 13, 8277, 17720, 13, 50864],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.46666248028094953,\n",
       "   'compression_ratio': 0.8333333333333334,\n",
       "   'no_speech_prob': 0.00011331542918924242}],\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transcribe('downloaded', fp16=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Transcription to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text = model.transcribe('downloaded', fp16=False)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hey, what's up coders? Welcome to one little coder. The best speech to text in Python in 2022, I would say is open AI whisper. In less than three lines of Python code, you could have state of the art, machine learning, state of the art, deep learning model that can do ASR, automatic speech recognition or speech to text. In this video, I'm going to show you how you can have state of the art speech to text using Python in 2022. And let's get started. The first thing is this Google call up notebook will be in the YouTube description. All you have to do is expand the YouTube description, see the Google call up notebook, click open it and then you'll get to go. But if not, then if you're going to create your own Google call up notebook, just make sure that you're running GPU. There are two ways to make sure that you're running GPU. One click runtime and then click change runtime and you can see GPU accelerator. If not, run the first line NVIDIA SMI that will give you the configuration of the GPU that you're running. Currently, I've got a Tesla T4 which is most likely what you would get if you're running on Google call up. Otherwise, you also you can check the RAM memory offered where I've got a 16-github. So we're going to see how to use open AI whisper to do speech to text in Python. The first step is for us to make installation of the library. It's just one line of code, pip install, git place and directly the git repository and I'm installing it in quiet mode. At this point, we have successfully installed the library whisper that will help us do speech to text. Once we have the library installed, which is whisper, then we have to load the library and then we have to also load the model. So we have to load the library and we have to load the model. So import whisper will import whisper library and then whisper to load underscore model, then you can specify the model here. Now, what is the model that you want to specify? And that is quite something that it depends upon what you want to do. So this is the model called you can see there is like five different types of model. Tiny, base, small, medium, large and you can also see each of these models, how many parameters they have got. If you are not familiar with deep learning models, the larger the number of size of parameters that you have got, the better or more accurate that these models would be. So you can see 39 million parameters, 74 million parameters for base, 244 million parameters for small, 769 million parameters of a medium, 1.5 billion parameters for the large model. And you can also see that they have got specifically, they have got special English only model in case if you want English only or if you want multi-lingual model, you can have multi-lingual. So tiny.en will give you English only model, tiny alone will give you multi-lingual model, which means you can do speech to text for a lot of languages that they have described here in this chart. And what is the RAM, be RAM requirement if you are running it on Google, sorry if you're running it on GPU, you can see the approximately around 1GB for tiny model, it goes all the way up to 10 gig for the large model and 5 gig for the medium model. And you can also see the improvement in the speed that you would get the relative speed. So if large model is going to take 1x, then you can see how much the medium size model would take, how much the small size model would take, how much the base model would take and how much the tiny model would take. So the tiny model would have like 32% or sorry 32 times more efficiency in terms of the model speed, especially the inference speed compared to the large model. So based on what you want the accuracy or the speed, you can decide what model that you want to go with. Right now at this point, I'm going to pick the medium model. Also, it is quite important for you to understand what is the type of language that you're going to use and see the word error rate, WER, the word error rate for that particular type of the language and then make a call about which model that you want to pick. Because I'm going to do just English, I'm going to go ahead with medium, which is a 760, I think a 769 million parameter model. So load the medium model. If you want to load something else like tiny, you can just change the line here, tiny or base or whatever, you can change the line here. So at this point, it's going to download the model. You can see that the model is a 1.4 to gigabits model and the model is successfully downloaded into your Google Collapse session. Now we are ready to do speech to text and as you know, for any speech to text, you need an audio file or a speech file that you're going to convert into text. For that, I'm going to download the audio file from the internet. For that, I'm just simply using the bash command WGet and I'm outputting it in the file name audio.mp3 and then I'm specifying the file here. So this comes from this website and this is this is particularly from a movie called Batman Begin by Christopher Nolan and it is particularly downloading this particular statement that says criminals thrive on the indulgence of societies understanding. I mean, as a native English speaker, this is quite difficult for me to understand or transcribe. Let me play this audio for you. Okay, so the same audio is downloaded here using WGet and once you download it, you can actually see the audio here in the folder and the next thing that you can do is play the same thing just to make sure that it is downloaded. Okay, at this point, we are ready to go to start our speech to text transcription. So we are going to do speech to text transcription and all you have to do is model dot transcribe and then do audio dot MP3. So model dot transcribe audio dot MP3 will give you the result and you can print the particular result, the text of the result. While you are printing this, while the model is transcribing, you can also get to know what language is this and also finally you can print the text. So when I print this, like you can see how long it takes, I am transcribing it audio dot MP3 and then the result is printed English. Criminals thrive on the indulgence of societies understanding. Now, at this point, you can stop the video, go ahead, build your own speech to text project. But if you want to know how the model sizes would differ and then that can have an impact, let me show you a quick demo. What I am going to do is I am going to just run the same thing. But instead of using a media model, I am going to use a tiny model. I am going to call it model underscore tiny or I can just simply call it tiny. And after I have the tiny model, I am going to use the same audio. I am not going to do anything different. I am going to use the same audio using tiny dot transcribe and when I say tiny and you can see that you downloaded so fast because the model is very small. And when I say tiny dot transcribe, you can see criminals thrive on the contents of societies understanding. You can see how the language is different first. It didn't understand criminals properly. There is an apostrophe S and also on the indulgence of societies understanding. So instead of indulgence, it says condense. So these words where there is either the noise or when the voice drops, the volume goes down. The speaker is not quite clear. You can see how the smaller size model performs versus the larger size model. Let us take another example. So I am going to go pick another example from the same movie. So you look very fashionable apart from the mud. I am going to copy the path. I am going to copy the path here. Copy the audio path. Come back here and then I am going to do the same thing that I just did last time. Oops. Yep. Mud.mp3. When we downloaded, we are going to save it still in the same file name audio.mp3. It's played. Oops. Did it not download? Let me delete this file just to make sure that this is gone. Come back here. Run this once again. Just to make sure that we are downloading the right file and played. You look very fashionable. And you can see the difference. Right? The first person who spoke spoken American accent. Now the person who spoke the spoken British accent. And this is Michael Kane. If you do not know. So now we are going to do transcription for that. First, we are going to use the larger model. In this case, we are going to use the model that is medium model. So it says you look very fashionable apart from the mud. I really suppose to be apart from the mud, but it says apart from the mud. Let's see what a tiny model does. It says you look very fashionable. Oh, pretty much. So you can see the last word was like the only one letter was the mistake in the previous model. But here you can see. Oh, pretty much. So you can see the huge difference. I would like to do one final test before we close the video. So this is the one. Bats are nocturnal. Bats might be, but even for billion playboys, three o'clock is pushing it. So I'm going to copy this and copy the audio address. Come back to the Google collab. First, delete the file in case, you know, if it is going to create any problem. Come back here, paste it, run this. So this is going to download the file after it downloads. I'm going to play it. I think I have to run it. I'm sorry. That's my mistake. Okay. Let's do the transcription. Bats are not pernil instead of saying nocturnal. Bats maybe, but even a billionaires playboy three o'clock is pushing him. Let's run this. English. That's your knock journal. That's my bite. But even for a billionaire playboy where the clock is pushing me, you can see the difference in the quality of transcription even for English when you change different models. But the point here is not to pick on the models, but the point here is you always have to make this call the trade of between the size of the model, the performance in when I say performance, the speed of the model's inference and also the error rate. So it's a call that you have to make, but it's my duty for me to tell you how the tiny model is performing versus how the large model is performing for the same audio clip. But if you just consider the medium model, which is not the largest model, this is the 769, 769 million parameter model. But even this model is quite amazing. It is not just simply amazing for the US accent, but you also saw that it was doing a good job for the UK accent, which is a British accent. And I myself have tested Indian accent with that. And then it has done a really good job even for Indian accent English, which is not something that I usually see with Google Assistant or Alexa or Siri. They usually struggle with Indian accent until, you know, it's really customized for Indian market. But otherwise, I usually see them struggling. But even then, this complete, like the simple open source model, open AI whisper is doing really a great job. So in 2022, if you want to build a speech to text solution using Python, I think you should turn towards opening a whisper. It's completely open source. You can see all the details about, you know, what is the error rate for, sorry, what is the error rate for each language? Word error rate, W here. What does the difference between each model and what other things that you can do? For example, translation and multilingual translation and language detection and all those things. And this, this particular project is aimed at somebody who doesn't know how to do speech to text. And the point is, just literally like three lines of Python code, like if you literally see the code, you import, then you load the model and then you can just literally, you know, transcribe and print the result. So basically in three lines of Python, let me run this for you. So I'm going to say three, sorry, three lines of Python code for speech to text. Okay. So what am I going to do? I'm going to say import whisper cool. Then model is equal to whisper.load underscore model. And I can say medium here. And then model dot transcribe of audio dot MP3. And then while I'm doing that, I can print the text. And when I run this, it should oops, there is load model. I'm so sorry load model. And while I'm running this, you can see at the end of this step, it is going to actually print the transcription of the audio. So in literally three lines of Python code, you can have a cutting edge state of the art speech to text model, a speech to text project in Python. And that is quite quite quite amazing. And I think you should definitely check this out. And if you are a university student trying to do something in speech to text, this is quite multi-lingual. Even if you're not trying to do something for English, there are a lot, lot more languages. You can see all the languages here. So I come from India, all these Indian languages are there, which is quite not not common with other other language models. So I would strongly recommend you to check this out. Any question, let me know in the comment section. Otherwise, see you in the next video. Happy coding.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"transcription.txt\", \"wt\")\n",
    "n = text_file.write(model_text)\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1131d84b9e97d700f196cec3f143c1c5ca4787d89ba01101505d30befb8a4c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
