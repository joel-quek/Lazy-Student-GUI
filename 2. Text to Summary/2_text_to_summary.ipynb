{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text To Summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.youtube.com/watch?v=TsfLm5iiYb4\n",
    "\n",
    "Title: AI Text Summarization with Hugging Face Transformers in 4 Lines of Python\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace Documentation: https://huggingface.co/docs/transformers/installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Transformers and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.1.97)\n",
      "Requirement already satisfied: boto3 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.27)\n",
      "Requirement already satisfied: tqdm in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: regex in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: requests in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.27 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers) (1.29.27)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: six in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->transformers) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.30.0,>=1.29.27->boto3->transformers) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (14.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.23.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (58.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (22.12.6)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install transformers[tf-cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: transformers 2.2.2 does not provide the extra 'torch'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: boto3 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (1.26.27)\n",
      "Requirement already satisfied: requests in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (2.28.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (0.1.97)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (0.0.53)\n",
      "Requirement already satisfied: tqdm in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (4.64.1)\n",
      "Requirement already satisfied: regex in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (2022.9.13)\n",
      "Requirement already satisfied: numpy in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (1.23.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers[torch]) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.27 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers[torch]) (1.29.27)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers[torch]) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (1.26.12)\n",
      "Requirement already satisfied: six in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers[torch]) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers[torch]) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.30.0,>=1.29.27->boto3->transformers[torch]) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[flax] in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[flax]) (1.23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[flax]) (2.28.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[flax]) (1.26.27)\n",
      "Requirement already satisfied: tqdm in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[flax]) (4.64.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[flax]) (0.0.53)\n",
      "Requirement already satisfied: regex in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[flax]) (2022.9.13)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[flax]) (0.1.97)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.27 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers[flax]) (1.29.27)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers[flax]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers[flax]) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[flax]) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[flax]) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[flax]) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[flax]) (2022.9.24)\n",
      "Requirement already satisfied: six in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers[flax]) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers[flax]) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers[flax]) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->transformers[flax]) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.30.0,>=1.29.27->boto3->transformers[flax]) (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: transformers 2.2.2 does not provide the extra 'flax'\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install transformers[flax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[pipeline] in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[pipeline]) (0.1.97)\n",
      "Requirement already satisfied: numpy in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[pipeline]) (1.23.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[pipeline]) (4.64.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[pipeline]) (0.0.53)\n",
      "Requirement already satisfied: regex in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[pipeline]) (2022.9.13)\n",
      "Requirement already satisfied: boto3 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[pipeline]) (1.26.27)\n",
      "Requirement already satisfied: requests in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[pipeline]) (2.28.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers[pipeline]) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.27 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers[pipeline]) (1.29.27)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from boto3->transformers[pipeline]) (1.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[pipeline]) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[pipeline]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[pipeline]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[pipeline]) (1.26.12)\n",
      "Requirement already satisfied: six in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers[pipeline]) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers[pipeline]) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers[pipeline]) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->transformers[pipeline]) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from botocore<1.30.0,>=1.29.27->boto3->transformers[pipeline]) (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: transformers 2.2.2 does not provide the extra 'pipeline'\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install transformers[pipeline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\users\\redoc\\appdata\\local\\temp\\pip-req-build-xwcb83d9\n",
      "  Resolved https://github.com/huggingface/transformers to commit 799cea64ac1029d66e9e58f18bc6f47892270723\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.26.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.26.0.dev0) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.26.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.26.0.dev0) (2022.9.13)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.26.0.dev0) (1.23.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.26.0.dev0) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.26.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.26.0.dev0) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.26.0.dev0) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.26.0.dev0) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers==4.26.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers==4.26.0.dev0) (0.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.26.0.dev0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.26.0.dev0) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.26.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.26.0.dev0) (3.4)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.26.0.dev0-py3-none-any.whl size=5949068 sha256=43ac8a5ef150dc78c0c2ca856514faf599159bfc1acd05e7dc54adf7e3aa537f\n",
      "  Stored in directory: C:\\Users\\redoc\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-eu2vvbzh\\wheels\\c0\\14\\d6\\6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.2.2\n",
      "    Uninstalling transformers-2.2.2:\n",
      "      Successfully uninstalled transformers-2.2.2\n",
      "Successfully installed transformers-4.26.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\redoc\\AppData\\Local\\Temp\\pip-req-build-xwcb83d9'\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.11.3\n",
      "  Using cached transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.11.3) (1.23.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.11.3) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.11.3) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.11.3) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.11.3) (2022.9.13)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.11.3) (0.0.53)\n",
      "Requirement already satisfied: requests in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.11.3) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.11.3) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers==4.11.3) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers==4.11.3) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers==4.11.3) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.11.3) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.11.3) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.11.3) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers==4.11.3) (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers==4.11.3) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers==4.11.3) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\redoc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sacremoses->transformers==4.11.3) (1.2.0)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [51 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-310\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\n",
      "      copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
      "      copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-310\\tokenizers\\tools\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install transformers==4.11.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I CANT IMPORT PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially kept saying \"ImportError: cannot import name 'pipeline' from 'transformers'\" but now it works. Strange but whatever."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Summarization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e195ba9e71444bb5bb4a822abb302866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\redoc\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45791c7c74f1442d99f2816b40b8ad7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157979f923114c608339f084479ac664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958879d1b5d040769d1a113c6be5a6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf9e95b4c51496b9532095b1234cbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transcription.txt') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Hey, what's up coders? Welcome to one little coder. The best speech to text in Python in 2022, I would say is open AI whisper. In less than three lines of Python code, you could have state of the art, machine learning, state of the art, deep learning model that can do ASR, automatic speech recognition or speech to text. In this video, I'm going to show you how you can have state of the art speech to text using Python in 2022. And let's get started. The first thing is this Google call up notebook will be in the YouTube description. All you have to do is expand the YouTube description, see the Google call up notebook, click open it and then you'll get to go. But if not, then if you're going to create your own Google call up notebook, just make sure that you're running GPU. There are two ways to make sure that you're running GPU. One click runtime and then click change runtime and you can see GPU accelerator. If not, run the first line NVIDIA SMI that will give you the configuration of the GPU that you're running. Currently, I've got a Tesla T4 which is most likely what you would get if you're running on Google call up. Otherwise, you also you can check the RAM memory offered where I've got a 16-github. So we're going to see how to use open AI whisper to do speech to text in Python. The first step is for us to make installation of the library. It's just one line of code, pip install, git place and directly the git repository and I'm installing it in quiet mode. At this point, we have successfully installed the library whisper that will help us do speech to text. Once we have the library installed, which is whisper, then we have to load the library and then we have to also load the model. So we have to load the library and we have to load the model. So import whisper will import whisper library and then whisper to load underscore model, then you can specify the model here. Now, what is the model that you want to specify? And that is quite something that it depends upon what you want to do. So this is the model called you can see there is like five different types of model. Tiny, base, small, medium, large and you can also see each of these models, how many parameters they have got. If you are not familiar with deep learning models, the larger the number of size of parameters that you have got, the better or more accurate that these models would be. So you can see 39 million parameters, 74 million parameters for base, 244 million parameters for small, 769 million parameters of a medium, 1.5 billion parameters for the large model. And you can also see that they have got specifically, they have got special English only model in case if you want English only or if you want multi-lingual model, you can have multi-lingual. So tiny.en will give you English only model, tiny alone will give you multi-lingual model, which means you can do speech to text for a lot of languages that they have described here in this chart. And what is the RAM, be RAM requirement if you are running it on Google, sorry if you're running it on GPU, you can see the approximately around 1GB for tiny model, it goes all the way up to 10 gig for the large model and 5 gig for the medium model. And you can also see the improvement in the speed that you would get the relative speed. So if large model is going to take 1x, then you can see how much the medium size model would take, how much the small size model would take, how much the base model would take and how much the tiny model would take. So the tiny model would have like 32% or sorry 32 times more efficiency in terms of the model speed, especially the inference speed compared to the large model. So based on what you want the accuracy or the speed, you can decide what model that you want to go with. Right now at this point, I'm going to pick the medium model. Also, it is quite important for you to understand what is the type of language that you're going to use and see the word error rate, WER, the word error rate for that particular type of the language and then make a call about which model that you want to pick. Because I'm going to do just English, I'm going to go ahead with medium, which is a 760, I think a 769 million parameter model. So load the medium model. If you want to load something else like tiny, you can just change the line here, tiny or base or whatever, you can change the line here. So at this point, it's going to download the model. You can see that the model is a 1.4 to gigabits model and the model is successfully downloaded into your Google Collapse session. Now we are ready to do speech to text and as you know, for any speech to text, you need an audio file or a speech file that you're going to convert into text. For that, I'm going to download the audio file from the internet. For that, I'm just simply using the bash command WGet and I'm outputting it in the file name audio.mp3 and then I'm specifying the file here. So this comes from this website and this is this is particularly from a movie called Batman Begin by Christopher Nolan and it is particularly downloading this particular statement that says criminals thrive on the indulgence of societies understanding. I mean, as a native English speaker, this is quite difficult for me to understand or transcribe. Let me play this audio for you. Okay, so the same audio is downloaded here using WGet and once you download it, you can actually see the audio here in the folder and the next thing that you can do is play the same thing just to make sure that it is downloaded. Okay, at this point, we are ready to go to start our speech to text transcription. So we are going to do speech to text transcription and all you have to do is model dot transcribe and then do audio dot MP3. So model dot transcribe audio dot MP3 will give you the result and you can print the particular result, the text of the result. While you are printing this, while the model is transcribing, you can also get to know what language is this and also finally you can print the text. So when I print this, like you can see how long it takes, I am transcribing it audio dot MP3 and then the result is printed English. Criminals thrive on the indulgence of societies understanding. Now, at this point, you can stop the video, go ahead, build your own speech to text project. But if you want to know how the model sizes would differ and then that can have an impact, let me show you a quick demo. What I am going to do is I am going to just run the same thing. But instead of using a media model, I am going to use a tiny model. I am going to call it model underscore tiny or I can just simply call it tiny. And after I have the tiny model, I am going to use the same audio. I am not going to do anything different. I am going to use the same audio using tiny dot transcribe and when I say tiny and you can see that you downloaded so fast because the model is very small. And when I say tiny dot transcribe, you can see criminals thrive on the contents of societies understanding. You can see how the language is different first. It didn't understand criminals properly. There is an apostrophe S and also on the indulgence of societies understanding. So instead of indulgence, it says condense. So these words where there is either the noise or when the voice drops, the volume goes down. The speaker is not quite clear. You can see how the smaller size model performs versus the larger size model. Let us take another example. So I am going to go pick another example from the same movie. So you look very fashionable apart from the mud. I am going to copy the path. I am going to copy the path here. Copy the audio path. Come back here and then I am going to do the same thing that I just did last time. Oops. Yep. Mud.mp3. When we downloaded, we are going to save it still in the same file name audio.mp3. It's played. Oops. Did it not download? Let me delete this file just to make sure that this is gone. Come back here. Run this once again. Just to make sure that we are downloading the right file and played. You look very fashionable. And you can see the difference. Right? The first person who spoke spoken American accent. Now the person who spoke the spoken British accent. And this is Michael Kane. If you do not know. So now we are going to do transcription for that. First, we are going to use the larger model. In this case, we are going to use the model that is medium model. So it says you look very fashionable apart from the mud. I really suppose to be apart from the mud, but it says apart from the mud. Let's see what a tiny model does. It says you look very fashionable. Oh, pretty much. So you can see the last word was like the only one letter was the mistake in the previous model. But here you can see. Oh, pretty much. So you can see the huge difference. I would like to do one final test before we close the video. So this is the one. Bats are nocturnal. Bats might be, but even for billion playboys, three o'clock is pushing it. So I'm going to copy this and copy the audio address. Come back to the Google collab. First, delete the file in case, you know, if it is going to create any problem. Come back here, paste it, run this. So this is going to download the file after it downloads. I'm going to play it. I think I have to run it. I'm sorry. That's my mistake. Okay. Let's do the transcription. Bats are not pernil instead of saying nocturnal. Bats maybe, but even a billionaires playboy three o'clock is pushing him. Let's run this. English. That's your knock journal. That's my bite. But even for a billionaire playboy where the clock is pushing me, you can see the difference in the quality of transcription even for English when you change different models. But the point here is not to pick on the models, but the point here is you always have to make this call the trade of between the size of the model, the performance in when I say performance, the speed of the model's inference and also the error rate. So it's a call that you have to make, but it's my duty for me to tell you how the tiny model is performing versus how the large model is performing for the same audio clip. But if you just consider the medium model, which is not the largest model, this is the 769, 769 million parameter model. But even this model is quite amazing. It is not just simply amazing for the US accent, but you also saw that it was doing a good job for the UK accent, which is a British accent. And I myself have tested Indian accent with that. And then it has done a really good job even for Indian accent English, which is not something that I usually see with Google Assistant or Alexa or Siri. They usually struggle with Indian accent until, you know, it's really customized for Indian market. But otherwise, I usually see them struggling. But even then, this complete, like the simple open source model, open AI whisper is doing really a great job. So in 2022, if you want to build a speech to text solution using Python, I think you should turn towards opening a whisper. It's completely open source. You can see all the details about, you know, what is the error rate for, sorry, what is the error rate for each language? Word error rate, W here. What does the difference between each model and what other things that you can do? For example, translation and multilingual translation and language detection and all those things. And this, this particular project is aimed at somebody who doesn't know how to do speech to text. And the point is, just literally like three lines of Python code, like if you literally see the code, you import, then you load the model and then you can just literally, you know, transcribe and print the result. So basically in three lines of Python, let me run this for you. So I'm going to say three, sorry, three lines of Python code for speech to text. Okay. So what am I going to do? I'm going to say import whisper cool. Then model is equal to whisper.load underscore model. And I can say medium here. And then model dot transcribe of audio dot MP3. And then while I'm doing that, I can print the text. And when I run this, it should oops, there is load model. I'm so sorry load model. And while I'm running this, you can see at the end of this step, it is going to actually print the transcription of the audio. So in literally three lines of Python code, you can have a cutting edge state of the art speech to text model, a speech to text project in Python. And that is quite quite quite amazing. And I think you should definitely check this out. And if you are a university student trying to do something in speech to text, this is quite multi-lingual. Even if you're not trying to do something for English, there are a lot, lot more languages. You can see all the languages here. So I come from India, all these Indian languages are there, which is quite not not common with other other language models. So I would strongly recommend you to check this out. Any question, let me know in the comment section. Otherwise, see you in the next video. Happy coding.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hey, what's up coders? Welcome to one little coder. The best speech to text in Python in 2022, I would say is open AI whisper. In less than three lines of Python code, you could have state of the art, machine learning, state of the art, deep learning model that can do ASR, automatic speech recognition or speech to text. In this video, I'm going to show you how you can have state of the art speech to text using Python in 2022. And let's get started. The first thing is this Google call up notebook will be in the YouTube description. All you have to do is expand the YouTube description, see the Google call up notebook, click open it and then you'll get to go. But if not, then if you're going to create your own Google call up notebook, just make sure that you're running GPU. There are two ways to make sure that you're running GPU. One click runtime and then click change runtime and you can see GPU accelerator. If not, run the first line NVIDIA SMI that will give you the configuration of the GPU that you're running. Currently, I've got a Tesla T4 which is most likely what you would get if you're running on Google call up. Otherwise, you also you can check the RAM memory offered where I've got a 16-github. So we're going to see how to use open AI whisper to do speech to text in Python. The first step is for us to make installation of the library. It's just one line of code, pip install, git place and directly the git repository and I'm installing it in quiet mode. At this point, we have successfully installed the library whisper that will help us do speech to text. Once we have the library installed, which is whisper, then we have to load the library and then we have to also load the model. So we have to load the library and we have to load the model. So import whisper will import whisper library and then whisper to load underscore model, then you can specify the model here. Now, what is the model that you want to specify? And that is quite something that it depends upon what you want to do. So this is the model called you can see there is like five different types of model. Tiny, base, small, medium, large and you can also see each of these models, how many parameters they have got. If you are not familiar with deep learning models, the larger the number of size of parameters that you have got, the better or more accurate that these models would be. So you can see 39 million parameters, 74 million parameters for base, 244 million parameters for small, 769 million parameters of a medium, 1.5 billion parameters for the large model. And you can also see that they have got specifically, they have got special English only model in case if you want English only or if you want multi-lingual model, you can have multi-lingual. So tiny.en will give you English only model, tiny alone will give you multi-lingual model, which means you can do speech to text for a lot of languages that they have described here in this chart. And what is the RAM, be RAM requirement if you are running it on Google, sorry if you're running it on GPU, you can see the approximately around 1GB for tiny model, it goes all the way up to 10 gig for the large model and 5 gig for the medium model. And you can also see the improvement in the speed that you would get the relative speed. So if large model is going to take 1x, then you can see how much the medium size model would take, how much the small size model would take, how much the base model would take and how much the tiny model would take. So the tiny model would have like 32% or sorry 32 times more efficiency in terms of the model speed, especially the inference speed compared to the large model. So based on what you want the accuracy or the speed, you can decide what model that you want to go with. Right now at this point, I'm going to pick the medium model. Also, it is quite important for you to understand what is the type of language that you're going to use and see the word error rate, WER, the word error rate for that particular type of the language and then make a call about which model that you want to pick. Because I'm going to do just English, I'm going to go ahead with medium, which is a 760, I think a 769 million parameter model. So load the medium model. If you want to load something else like tiny, you can just change the line here, tiny or base or whatever, you can change the line here. So at this point, it's going to download the model. You can see that the model is a 1.4 to gigabits model and the model is successfully downloaded into your Google Collapse session. Now we are ready to do speech to text and as you know, for any speech to text, you need an audio file or a speech file that you're going to convert into text. For that, I'm going to download the audio file from the internet. For that, I'm just simply using the bash command WGet and I'm outputting it in the file name audio.mp3 and then I'm specifying the file here. So this comes from this website and this is this is particularly from a movie called Batman Begin by Christopher Nolan and it is particularly downloading this particular statement that says criminals thrive on the indulgence of societies understanding. I mean, as a native English speaker, this is quite difficult for me to understand or transcribe. Let me play this audio for you. Okay, so the same audio is downloaded here using WGet and once you download it, you can actually see the audio here in the folder and the next thing that you can do is play the same thing just to make sure that it is downloaded. Okay, at this point, we are ready to go to start our speech to text transcription. So we are going to do speech to text transcription and all you have to do is model dot transcribe and then do audio dot MP3. So model dot transcribe audio dot MP3 will give you the result and you can print the particular result, the text of the result. While you are printing this, while the model is transcribing, you can also get to know what language is this and also finally you can print the text. So when I print this, like you can see how long it takes, I am transcribing it audio dot MP3 and then the result is printed English. Criminals thrive on the indulgence of societies understanding. Now, at this point, you can stop the video, go ahead, build your own speech to text project. But if you want to know how the model sizes would differ and then that can have an impact, let me show you a quick demo. What I am going to do is I am going to just run the same thing. But instead of using a media model, I am going to use a tiny model. I am going to call it model underscore tiny or I can just simply call it tiny. And after I have the tiny model, I am going to use the same audio. I am not going to do anything different. I am going to use the same audio using tiny dot transcribe and when I say tiny and you can see that you downloaded so fast because the model is very small. And when I say tiny dot transcribe, you can see criminals thrive on the contents of societies understanding. You can see how the language is different first. It didn't understand criminals properly. There is an apostrophe S and also on the indulgence of societies understanding. So instead of indulgence, it says condense. So these words where there is either the noise or when the voice drops, the volume goes down. The speaker is not quite clear. You can see how the smaller size model performs versus the larger size model. Let us take another example. So I am going to go pick another example from the same movie. So you look very fashionable apart from the mud. I am going to copy the path. I am going to copy the path here. Copy the audio path. Come back here and then I am going to do the same thing that I just did last time. Oops. Yep. Mud.mp3. When we downloaded, we are going to save it still in the same file name audio.mp3. It's played. Oops. Did it not download? Let me delete this file just to make sure that this is gone. Come back here. Run this once again. Just to make sure that we are downloading the right file and played. You look very fashionable. And you can see the difference. Right? The first person who spoke spoken American accent. Now the person who spoke the spoken British accent. And this is Michael Kane. If you do not know. So now we are going to do transcription for that. First, we are going to use the larger model. In this case, we are going to use the model that is medium model. So it says you look very fashionable apart from the mud. I really suppose to be apart from the mud, but it says apart from the mud. Let's see what a tiny model does. It says you look very fashionable. Oh, pretty much. So you can see the last word was like the only one letter was the mistake in the previous model. But here you can see. Oh, pretty much. So you can see the huge difference. I would like to do one final test before we close the video. So this is the one. Bats are nocturnal. Bats might be, but even for billion playboys, three o'clock is pushing it. So I'm going to copy this and copy the audio address. Come back to the Google collab. First, delete the file in case, you know, if it is going to create any problem. Come back here, paste it, run this. So this is going to download the file after it downloads. I'm going to play it. I think I have to run it. I'm sorry. That's my mistake. Okay. Let's do the transcription. Bats are not pernil instead of saying nocturnal. Bats maybe, but even a billionaires playboy three o'clock is pushing him. Let's run this. English. That's your knock journal. That's my bite. But even for a billionaire playboy where the clock is pushing me, you can see the difference in the quality of transcription even for English when you change different models. But the point here is not to pick on the models, but the point here is you always have to make this call the trade of between the size of the model, the performance in when I say performance, the speed of the model's inference and also the error rate. So it's a call that you have to make, but it's my duty for me to tell you how the tiny model is performing versus how the large model is performing for the same audio clip. But if you just consider the medium model, which is not the largest model, this is the 769, 769 million parameter model. But even this model is quite amazing. It is not just simply amazing for the US accent, but you also saw that it was doing a good job for the UK accent, which is a British accent. And I myself have tested Indian accent with that. And then it has done a really good job even for Indian accent English, which is not something that I usually see with Google Assistant or Alexa or Siri. They usually struggle with Indian accent until, you know, it's really customized for Indian market. But otherwise, I usually see them struggling. But even then, this complete, like the simple open source model, open AI whisper is doing really a great job. So in 2022, if you want to build a speech to text solution using Python, I think you should turn towards opening a whisper. It's completely open source. You can see all the details about, you know, what is the error rate for, sorry, what is the error rate for each language? Word error rate, W here. What does the difference between each model and what other things that you can do? For example, translation and multilingual translation and language detection and all those things. And this, this particular project is aimed at somebody who doesn't know how to do speech to text. And the point is, just literally like three lines of Python code, like if you literally see the code, you import, then you load the model and then you can just literally, you know, transcribe and print the result. So basically in three lines of Python, let me run this for you. So I'm going to say three, sorry, three lines of Python code for speech to text. Okay. So what am I going to do? I'm going to say import whisper cool. Then model is equal to whisper.load underscore model. And I can say medium here. And then model dot transcribe of audio dot MP3. And then while I'm doing that, I can print the text. And when I run this, it should oops, there is load model. I'm so sorry load model. And while I'm running this, you can see at the end of this step, it is going to actually print the transcription of the audio. So in literally three lines of Python code, you can have a cutting edge state of the art speech to text model, a speech to text project in Python. And that is quite quite quite amazing. And I think you should definitely check this out. And if you are a university student trying to do something in speech to text, this is quite multi-lingual. Even if you're not trying to do something for English, there are a lot, lot more languages. You can see all the languages here. So I come from India, all these Indian languages are there, which is quite not not common with other other language models. So I would strongly recommend you to check this out. Any question, let me know in the comment section. Otherwise, see you in the next video. Happy coding.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(lines[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hey, what's up coders? Welcome to one little coder. The best speech to text in Python in 2022, I wo\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0][0:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\redoc\\OneDrive\\Desktop\\DSI-Roughpaper\\0. The_Lazy_Student\\2. Text to Summary\\2_text_to_summary.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/redoc/OneDrive/Desktop/DSI-Roughpaper/0.%20The_Lazy_Student/2.%20Text%20to%20Summary/2_text_to_summary.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m summary_text\u001b[39m=\u001b[39msummarizer(lines[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:265\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    242\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m          ids of the summary.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    137\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    167\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[0;32m    168\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[0;32m    169\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[0;32m    170\u001b[0m     ):\n\u001b[0;32m    171\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1074\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1081\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1080\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1081\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1082\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1083\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m    989\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 990\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m    991\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[0;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 187\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    188\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1367\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1360\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1361\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1362\u001b[0m         )\n\u001b[0;32m   1364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1365\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1367\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m   1368\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[0;32m   1369\u001b[0m     )\n\u001b[0;32m   1371\u001b[0m \u001b[39m# 4. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:601\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    599\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    600\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 601\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    603\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:811\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens(input_ids) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_scale\n\u001b[1;32m--> 811\u001b[0m embed_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_positions(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    812\u001b[0m embed_pos \u001b[39m=\u001b[39m embed_pos\u001b[39m.\u001b[39mto(inputs_embeds\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    814\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m embed_pos\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:139\u001b[0m, in \u001b[0;36mBartLearnedPositionalEmbedding.forward\u001b[1;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[0;32m    134\u001b[0m bsz, seq_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    135\u001b[0m positions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\n\u001b[0;32m    136\u001b[0m     past_key_values_length, past_key_values_length \u001b[39m+\u001b[39m seq_len, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    137\u001b[0m )\u001b[39m.\u001b[39mexpand(bsz, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(positions \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moffset)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "summary_text=summarizer(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\redoc\\OneDrive\\Desktop\\DSI-Roughpaper\\0. The_Lazy_Student\\2. Text to Summary\\2_text_to_summary.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/redoc/OneDrive/Desktop/DSI-Roughpaper/0.%20The_Lazy_Student/2.%20Text%20to%20Summary/2_text_to_summary.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m summary_text \u001b[39m=\u001b[39m summarizer(lines[\u001b[39m0\u001b[39;49m], max_length\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, min_length\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:265\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    242\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m          ids of the summary.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    137\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    167\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[0;32m    168\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[0;32m    169\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[0;32m    170\u001b[0m     ):\n\u001b[0;32m    171\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1074\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1081\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1080\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1081\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1082\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1083\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m    989\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 990\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m    991\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[0;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 187\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    188\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1367\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1360\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1361\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1362\u001b[0m         )\n\u001b[0;32m   1364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1365\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1367\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m   1368\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[0;32m   1369\u001b[0m     )\n\u001b[0;32m   1371\u001b[0m \u001b[39m# 4. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:601\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    599\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    600\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 601\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    603\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:811\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens(input_ids) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_scale\n\u001b[1;32m--> 811\u001b[0m embed_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_positions(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    812\u001b[0m embed_pos \u001b[39m=\u001b[39m embed_pos\u001b[39m.\u001b[39mto(inputs_embeds\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    814\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m embed_pos\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:139\u001b[0m, in \u001b[0;36mBartLearnedPositionalEmbedding.forward\u001b[1;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[0;32m    134\u001b[0m bsz, seq_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    135\u001b[0m positions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\n\u001b[0;32m    136\u001b[0m     past_key_values_length, past_key_values_length \u001b[39m+\u001b[39m seq_len, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    137\u001b[0m )\u001b[39m.\u001b[39mexpand(bsz, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(positions \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moffset)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "#summary_text = summarizer(lines[0], max_length=1000, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['num_sentences'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\redoc\\OneDrive\\Desktop\\DSI-Roughpaper\\0. The_Lazy_Student\\2. Text to Summary\\2_text_to_summary.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/redoc/OneDrive/Desktop/DSI-Roughpaper/0.%20The_Lazy_Student/2.%20Text%20to%20Summary/2_text_to_summary.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m summarizer(lines[\u001b[39m0\u001b[39;49m], num_sentences\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, min_length\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:265\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    242\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m          ids of the summary.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:165\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    137\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    167\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[0;32m    168\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[0;32m    169\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[0;32m    170\u001b[0m     ):\n\u001b[0;32m    171\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1074\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1081\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1080\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1081\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1082\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1083\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m    989\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 990\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m    991\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:187\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length)\n\u001b[0;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(input_length, generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m], generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 187\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    188\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1296\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[39m# 0. Validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_model_class()\n\u001b[1;32m-> 1296\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[0;32m   1298\u001b[0m \u001b[39m# 1. Set generation parameters if not already defined\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m bos_token_id \u001b[39m=\u001b[39m bos_token_id \u001b[39mif\u001b[39;00m bos_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mbos_token_id\n",
      "File \u001b[1;32mc:\\Users\\redoc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:993\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[1;34m(self, model_kwargs)\u001b[0m\n\u001b[0;32m    990\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[0;32m    992\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[1;32m--> 993\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    994\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    995\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    996\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['num_sentences'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "#result = summarizer(lines[0], num_sentences=5, min_length=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13085"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' In less than three lines of Python code, you could have state of the art, machine learning, deep learning model that can do ASR, automatic speech recognition or speech to text .'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(lines[0][0:1000], max_length=130, min_length=30,do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' In less than three lines of Python code, you could have state of the art, machine learning, deep learning model that can do ASR, automatic speech recognition or speech to text .'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(lines[0][0:2000], max_length=130, min_length=30,do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The best speech to text in Python in 2022, I would say is open AI whisper. In less than three lines of Python code, you could have state of the art, machine learning, deep learning model that can do ASR, automatic speech recognition .'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(lines[0][0:3000], max_length=130, min_length=30,do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \" In less than three lines of Python code, you could have state of the art, machine learning, deep learning model that can do ASR, automatic speech recognition or speech to text . In this video, we're going to see how to use open AI whisper to do speech in Python in 2022 .\"}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(lines[0][0:4000], max_length=130, min_length=30,do_sample=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Output Summary to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"summary.txt\", \"wt\")\n",
    "n = text_file.write(summary_text)\n",
    "text_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Here Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"\n",
    "Looking for a road block early on in your startup? When it comes to the nitty, gritty details of the backend, non-technical startup entrepreneurs underestimate the importance of learning to code far too often. Of course, they’re experts in non-technical areas like finance or marketing, and as it’s gotten them this far, they might not have considered the value in knowing essential programming fundamentals. As such, they often entrust software development to a CTO, hire freelancers or an agency.\n",
    "\n",
    "However, the founder’s lack of technical expertise sometimes causes inefficiencies when it comes to decisions in product architecture or design.\n",
    "\n",
    "Simply having a basic understanding of programming fundamentals and how the web works can be helpful in all stages of your business, clarifying many things that might have seemed mysterious before.\n",
    "\n",
    "Turning that basic understanding into a much more high-level knowledge does even more good. Specifically — to name a few things — it will allow you to map the product specs, create mockups and wireframes, estimate the resources needed to put the MVP together, understand what expertise is required for the project, and what technology will be the most practical to use.\n",
    "\n",
    "Now, you don’t need to become an expert in software development, but having a general understanding of what happens behind the scenes when you browse the website or a mobile app is a must if you are up to building a truly successful tech startup. Here’s why:\n",
    "\n",
    "1. Turning ideas into reality\n",
    "\n",
    "\n",
    "image\n",
    "\n",
    "\n",
    "If you’re planning to create a revolutionary platform that will disrupt the entirety of the digital assets market, awesome! Go you!\n",
    "\n",
    "But… that might be difficult if you have just a basic set of coding skills. It’d likely be much more practical to let professionals build the actual app.\n",
    "\n",
    "However, because you have that set of skills, you can create a landing page, describe what your app can do, and even let people leave their emails for when they either happen to like your idea or would like to become the first beta testers.\n",
    "\n",
    "Know more than a bit of HTML and CSS? Cool! You might be able to prove the fundamental concepts of your app before going further with development. That way you can make adjustments in the business strategy or the product design. You can build small prototypes intended to test how a specific technology or feature works yourself.\n",
    "\n",
    "As a plus, you will understand how to make an app from the beginning to the end.\n",
    "\n",
    "More than that, you might find yourself among the luckiest of us that end up raising funds or winning competitions with only a simple prototype you built yourself without hiring anyone from the outside.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' A basic understanding of programming fundamentals and how the web works can be helpful in all stages of your business . It will allow you to map the product specs, create mockups and wireframes, estimate the resources needed to put the MVP together .'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(article, max_length=130, min_length=30,do_sample=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kept Showing \"IndexError: index out of range in self.\" because my articles were too long. Find a way to shorten them or for Summarizer to work on larger articles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1131d84b9e97d700f196cec3f143c1c5ca4787d89ba01101505d30befb8a4c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
